Directory structure:
└── open-deep-research/
    ├── README.md
    ├── MANIFEST.in
    ├── requirements.txt
    ├── setup.py
    ├── cli/
    │   ├── gaia-eval-cli.py
    │   ├── research-agent-cli.py
    │   └── search-cli.py
    ├── handbooks/
    │   └── SMOLAGENTS.md
    ├── src/
    │   └── open_deep_research/
    │       ├── __init__.py
    │       ├── cookies.py
    │       ├── mdconvert.py
    │       ├── reformulator.py
    │       ├── run_agents.py
    │       ├── text_inspector_tool.py
    │       ├── text_web_browser.py
    │       └── visual_qa.py
    ├── tests/
    │   └── test_agent.py
    └── .claude/
        └── settings.local.json

================================================
FILE: README.md
================================================
# Open Deep Research

Welcome to this open replication of [OpenAI's Deep Research](https://openai.com/index/introducing-deep-research/)! This agent attempts to replicate OpenAI's model and achieve similar performance on research tasks.

Read more about this implementation's goal and methods in our [blog post](https://huggingface.co/blog/open-deep-research).


This agent achieves **55% pass@1** on the GAIA validation set, compared to **67%** for the original Deep Research.

## Setup

To get started, follow the steps below:

### Clone the repository

```bash
git clone https://github.com/pminervini/open-deep-research.git
cd open-deep-research
```

### Install dependencies

Run the following command to install the required dependencies from the `requirements.txt` file:

```bash
pip install -r requirements.txt
```

### Set up environment variables

The agent uses the `GoogleSearchTool` for web search, which requires an environment variable with the corresponding API key, based on the selected provider:
- `SERPAPI_API_KEY` for SerpApi: [Sign up here to get a key](https://serpapi.com/users/sign_up)
- `SERPER_API_KEY` for Serper: [Sign up here to get a key](https://serper.dev/signup)

Depending on the model you want to use, you may need to set environment variables.
For example, to use the default `gpt-4o-mini` model, you need to set the `OPENAI_API_KEY` environment variable.
[Sign up here to get a key](https://platform.openai.com/signup).


## Usage

Then you're good to go! Run the research agent script, as in:

### Using OpenAI or other standard providers
```bash
python cli/research-agent-cli.py --model "gpt-4o-mini" "2+2?"
```

### Using a local OpenAI-compatible endpoint
```bash
python cli/research-agent-cli.py --model "openai/qwen3:32b" --api-base "http://127.0.0.1:11434/v1" --api-key "dummy" "2+2?"
```

### CLI Options
- `--model`: Model name
- `--api-base`: Base URL for custom API endpoints (e.g., local LLM servers)
- `--api-key`: API key for authentication

## GAIA Evaluation

To run evaluation on the GAIA benchmark dataset, use the evaluation CLI:

### Using OpenAI or other standard providers
```bash
python cli/gaia-eval-cli.py --model "gpt-4o" --run-name "my-evaluation" --concurrency 8
```

### Using a local OpenAI-compatible endpoint
```bash
python cli/gaia-eval-cli.py --model "openai/qwen3:32b" --api-base "http://127.0.0.1:11434/v1" --api-key "dummy" --run-name "local-evaluation" --concurrency 4
```

### GAIA CLI Options
- `--model`: Model name
- `--api-base`: Base URL for custom API endpoints (e.g., local LLM servers)  
- `--api-key`: API key for authentication
- `--run-name`: Name for this evaluation run (required)
- `--concurrency`: Number of parallel tasks (default: 8)
- `--set-to-run`: Dataset split to evaluate ("validation" or "test", default: "validation")
- `--use-raw-dataset`: Use the raw GAIA dataset instead of the annotated version
- `--use-open-models`: Use open models (legacy option)

## Full reproducibility of results

The data used in our submissions to GAIA was augmented in this way:
 -  For each single-page .pdf or .xls file, it was opened in a file reader (MacOS Sonoma Numbers or Preview), and a ".png" screenshot was taken and added to the folder.
- Then for any file used in a question, the file loading system checks if there is a ".png" extension version of the file, and loads it instead of the original if it exists.

This process was done manually but could be automatized.

After processing, the annotated was uploaded to a [new dataset](https://huggingface.co/datasets/smolagents/GAIA-annotated). You need to request access (granted instantly).


================================================
FILE: MANIFEST.in
================================================
include README.md
include requirements.txt
include CLAUDE.md
include GEMINI.md
recursive-include handbooks *.md
recursive-include tests *.py
recursive-include cli *.py
prune downloads*
prune __pycache__
global-exclude *.pyc
global-exclude *.pyo
global-exclude *.orig
global-exclude *.rej


================================================
FILE: requirements.txt
================================================
# Core ML and AI frameworks
anthropic>=0.37.1
smolagents>=0.4.0
openai>=1.52.2
transformers>=4.46.0
torch>=2.2.2
torchvision>=0.17.2
huggingface_hub>=0.23.4

# Data processing and analysis
numpy>=2.1.2
pandas>=2.2.3
numexpr>=2.10.1
datasets>=2.21.0
scikit-learn>=1.3.0
scipy>=1.11.0

# Web scraping and search
beautifulsoup4>=4.12.3
google_search_results>=2.4.2
requests>=2.32.3
youtube_transcript_api>=0.6.2

# Document and file processing
# PDF processing
pdfminer>=20191125
pdfminer.six>=20240706
pypdf>=5.1.0
PyPDF2>=3.0.0

# Office documents
openpyxl>=3.1.0
python-pptx>=1.0.2
mammoth>=1.8.0

# Excel files
xlrd>=2.0.1

# Image processing
Pillow>=11.0.0

# Audio processing
pydub>=0.25.1
SpeechRecognition>=3.10.0
audioop-lts<1.0; python_version >= "3.13" # required to use pydub in Python >=3.13; LTS port of the removed Python builtin module audioop

# Text and markup processing
markdownify>=0.13.1

# File utilities
pathvalidate>=3.2.1
puremagic>=1.28

# Environment and configuration
python-dotenv>=1.0.1

# Scientific and specialized libraries
chess>=1.10.0
sympy>=1.12.0
pubchempy>=1.0.4
biopython>=1.83

# Utilities
tqdm>=4.66.4



================================================
FILE: setup.py
================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = [line.strip() for line in fh if line.strip() and not line.startswith("#")]

setup(
    name="open-deep-research",
    version="0.1.0",
    author="Pasquale Minervini",
    author_email="p.minervini@gmail.com",
    description="Open Deep Research",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/pminervini/open-deep-research",
    project_urls={
        "Bug Reports": "https://github.com/pminervini/open-deep-research/issues",
        "Source": "https://github.com/pminervini/open-deep-research",
        "Blog Post": "https://huggingface.co/blog/open-deep-research",
    },
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: Apache Software License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Topic :: Internet :: WWW/HTTP :: Browsers",
        "Topic :: Text Processing :: Indexing",
    ],
    python_requires=">=3.8",
    install_requires=requirements,
    extras_require={
        "dev": [
            "pytest>=7.0.0",
            "pytest-cov>=4.0.0",
            "ruff>=0.1.0",
            "black>=23.0.0",
        ],
        "test": [
            "pytest>=7.0.0",
            "pytest-cov>=4.0.0",
        ],
    },
    # entry_points={
    #     "console_scripts": [
    #         "open-deep-research=cli.research-agent-cli:main",
    #         "open-deep-research-eval=cli.gaia-eval-cli:main",
    #         "open-deep-research-search=cli.search-cli:main",
    #     ],
    # },
    include_package_data=True,
    zip_safe=False,
    keywords=[
        "artificial intelligence",
        "machine learning",
        "research",
        "web scraping",
        "multi-agent",
        "openai",
        "gaia",
        "benchmark",
    ],
)


================================================
FILE: cli/gaia-eval-cli.py
================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import json
import os
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any

import datasets
import pandas as pd
from dotenv import load_dotenv
from huggingface_hub import login, snapshot_download
from src.open_deep_research.reformulator import prepare_response
from src.open_deep_research.run_agents import get_single_file_description, get_zip_description
from src.open_deep_research.text_inspector_tool import TextInspectorTool
from src.open_deep_research.text_web_browser import ArchiveSearchTool, FinderTool, FindNextTool, PageDownTool, PageUpTool, SimpleTextBrowser, VisitTool
from src.open_deep_research.visual_qa import visualizer
from tqdm import tqdm

from smolagents import CodeAgent, GoogleSearchTool, LiteLLMModel, Model, ToolCallingAgent

load_dotenv(override=True)
login(os.getenv("HF_TOKEN"))

append_answer_lock = threading.Lock()

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--concurrency", type=int, default=8)
    parser.add_argument("--model", type=str, default="o1")
    parser.add_argument("--api-base", type=str, help="Base URL for the API endpoint")
    parser.add_argument("--api-key", type=str, help="API key for authentication")
    parser.add_argument("--run-name", type=str, required=True)
    parser.add_argument("--set-to-run", type=str, default="validation")
    parser.add_argument("--use-open-models", type=bool, default=False)
    parser.add_argument("--use-raw-dataset", action="store_true")
    parser.add_argument(
        "--manager-agent-type", 
        type=str, 
        choices=["CodeAgent", "ToolCallingAgent"], 
        default="CodeAgent",
        help="Type of agent to use for the manager (default: CodeAgent)"
    )
    parser.add_argument(
        "--search-agent-type", 
        type=str, 
        choices=["CodeAgent", "ToolCallingAgent"], 
        default="ToolCallingAgent",
        help="Type of agent to use for the search agent (default: ToolCallingAgent)"
    )
    return parser.parse_args()

### IMPORTANT: EVALUATION SWITCHES

print("Make sure you deactivated any VPN like Tailscale, else some URLs will be blocked!")

custom_role_conversions = {"tool-call": "assistant", "tool-response": "user"}

user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"

BROWSER_CONFIG = {
    "viewport_size": 1024 * 5,
    "downloads_folder": "downloads_folder",
    "request_kwargs": {
        "headers": {"User-Agent": user_agent},
        "timeout": 300,
    },
    "serpapi_key": os.getenv("SERPAPI_API_KEY"),
}

os.makedirs(f"./{BROWSER_CONFIG['downloads_folder']}", exist_ok=True)

def create_model(model_name: str, api_base: str = None, api_key: str = None) -> Model:
    """Create a model instance based on the provided parameters."""
    model_params = {
        "model_id": model_name,
        "custom_role_conversions": custom_role_conversions,
    }
    
    if api_base:
        model_params["api_base"] = api_base
    if api_key:
        model_params["api_key"] = api_key
    if model_name == "o1":
        model_params["reasoning_effort"] = "high"
        model_params["max_completion_tokens"] = 8192
    else:
        model_params["max_tokens"] = 4096
        
    return LiteLLMModel(**model_params)

def create_agent_team(model: Model, manager_agent_type: str = "CodeAgent", search_agent_type: str = "ToolCallingAgent"):
    text_limit = 100000
    ti_tool = TextInspectorTool(model, text_limit)

    browser = SimpleTextBrowser(**BROWSER_CONFIG)

    WEB_TOOLS = [
        GoogleSearchTool(provider="serper"),
        VisitTool(browser),
        PageUpTool(browser),
        PageDownTool(browser),
        FinderTool(browser),
        FindNextTool(browser),
        ArchiveSearchTool(browser),
        TextInspectorTool(model, text_limit),
    ]

    # Create search agent based on specified type
    search_agent_config = {
        "model": model,
        "max_steps": 20,
        "verbosity_level": 2,
        "planning_interval": 4,
        "name": "search_agent",
        "description": """A team member that will search the internet to answer your question.
    Ask him for all your questions that require browsing the web.
    Provide him as much context as possible, in particular if you need to search on a specific timeframe!
    And don't hesitate to provide him with a complex search task, like finding a difference between two webpages.
    Your request must be a real sentence, not a google search! Like "Find me this information (...)" rather than a few keywords.
    """,
        "provide_run_summary": True,
    }
    
    if search_agent_type == "ToolCallingAgent":
        search_agent_config["tools"] = WEB_TOOLS
        text_webbrowser_agent = ToolCallingAgent(**search_agent_config)
    else:  # CodeAgent
        search_agent_config["tools"] = WEB_TOOLS
        search_agent_config["additional_authorized_imports"] = ["*"]
        text_webbrowser_agent = CodeAgent(**search_agent_config)
    
    text_webbrowser_agent.prompt_templates["managed_agent"]["task"] += """You can navigate to .txt online files.
    If a non-html page is in another format, especially .pdf or a Youtube video, use tool 'inspect_file_as_text' to inspect it.
    Additionally, if after some searching you find out that you need more information to answer the question, you can use `final_answer` with your request for clarification as argument to request for more information."""

    # Create manager agent based on specified type
    manager_agent_config = {
        "model": model,
        "tools": [visualizer, ti_tool],
        "max_steps": 12,
        "verbosity_level": 2,
        "planning_interval": 4,
        "managed_agents": [text_webbrowser_agent],
    }
    
    if manager_agent_type == "CodeAgent":
        manager_agent_config["additional_authorized_imports"] = ["*"]
        manager_agent = CodeAgent(**manager_agent_config)
    else:  # ToolCallingAgent
        manager_agent = ToolCallingAgent(**manager_agent_config)
        
    return manager_agent

def load_gaia_dataset(use_raw_dataset: bool, set_to_run: str) -> datasets.Dataset:
    if not os.path.exists("data/gaia"):
        if use_raw_dataset:
            snapshot_download(
                repo_id="gaia-benchmark/GAIA",
                repo_type="dataset",
                local_dir="data/gaia",
                ignore_patterns=[".gitattributes", "README.md"],
            )
        else:
            # WARNING: this dataset is gated: make sure you visit the repo to require access.
            snapshot_download(
                repo_id="smolagents/GAIA-annotated",
                repo_type="dataset",
                local_dir="data/gaia",
                ignore_patterns=[".gitattributes", "README.md"],
            )

    def preprocess_file_paths(row):
        if len(row["file_name"]) > 0:
            row["file_name"] = f"data/gaia/{set_to_run}/" + row["file_name"]
        return row

    eval_ds = datasets.load_dataset(
        "data/gaia/GAIA.py",
        name="2023_all",
        split=set_to_run,
        # data_files={"validation": "validation/metadata.jsonl", "test": "test/metadata.jsonl"},
    )

    eval_ds = eval_ds.rename_columns({"Question": "question", "Final answer": "true_answer", "Level": "task"})
    eval_ds = eval_ds.map(preprocess_file_paths)
    return eval_ds

def append_answer(entry: dict, jsonl_file: str) -> None:
    jsonl_path = Path(jsonl_file)
    jsonl_path.parent.mkdir(parents=True, exist_ok=True)
    with append_answer_lock, open(jsonl_file, "a", encoding="utf-8") as fp:
        fp.write(json.dumps(entry) + "\n")
    assert jsonl_path.exists(), "File not found!"
    print("Answer exported to file:", jsonl_path.resolve())

def answer_single_question(
    example: dict, model_name: str, answers_file: str, visual_inspection_tool: TextInspectorTool, 
    api_base: str = None, api_key: str = None, manager_agent_type: str = "CodeAgent", search_agent_type: str = "ToolCallingAgent"
) -> None:
    model = create_model(model_name, api_base, api_key)
    # model = InferenceClientModel(model_id="Qwen/Qwen3-32B", provider="novita", max_tokens=4096)
    document_inspection_tool = TextInspectorTool(model, 100000)

    agent = create_agent_team(model, manager_agent_type, search_agent_type)

    augmented_question = """You have one question to answer. It is paramount that you provide a correct answer.
Give it all you can: I know for a fact that you have access to all the relevant tools to solve it and find the correct answer (the answer does exist).
Failure or 'I cannot answer' or 'None found' will not be tolerated, success will be rewarded.
Run verification steps if that's needed, you must make sure you find the correct answer! Here is the task:

""" + example["question"]

    if example["file_name"]:
        if ".zip" in example["file_name"]:
            prompt_use_files = "\n\nTo solve the task above, you will have to use these attached files:\n"
            prompt_use_files += get_zip_description(example["file_name"], example["question"], visual_inspection_tool, document_inspection_tool)
        else:
            prompt_use_files = "\n\nTo solve the task above, you will have to use this attached file:\n"
            prompt_use_files += get_single_file_description(example["file_name"], example["question"], visual_inspection_tool, document_inspection_tool)
        augmented_question += prompt_use_files

    start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        # Run agent
        final_result = agent.run(augmented_question)

        agent_memory = agent.write_memory_to_messages()

        final_result = prepare_response(augmented_question, agent_memory, reformulation_model=model)

        output = str(final_result)
        for memory_step in agent.memory.steps:
            memory_step.model_input_messages = None
        intermediate_steps = agent_memory

        # Check for parsing errors which indicate the LLM failed to follow the required format
        parsing_error = True if any(["AgentParsingError" in step for step in intermediate_steps]) else False

        # check if iteration limit exceeded
        iteration_limit_exceeded = True if "Agent stopped due to iteration limit or time limit." in output else False
        raised_exception = False

    except Exception as e:
        print("Error on ", augmented_question, e)
        output = None
        intermediate_steps = []
        parsing_error = False
        iteration_limit_exceeded = False
        exception = e
        raised_exception = True
    end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    token_counts_manager = agent.monitor.get_total_token_counts()
    token_counts_web = list(agent.managed_agents.values())[0].monitor.get_total_token_counts()
    total_token_counts = {
        "input": token_counts_manager["input"] + token_counts_web["input"],
        "output": token_counts_manager["output"] + token_counts_web["output"],
    }
    annotated_example = {
        "agent_name": model.model_id,
        "question": example["question"],
        "augmented_question": augmented_question,
        "prediction": output,
        "intermediate_steps": intermediate_steps,
        "parsing_error": parsing_error,
        "iteration_limit_exceeded": iteration_limit_exceeded,
        "agent_error": str(exception) if raised_exception else None,
        "task": example["task"],
        "task_id": example["task_id"],
        "true_answer": example["true_answer"],
        "start_time": start_time,
        "end_time": end_time,
        "token_counts": total_token_counts,
    }
    append_answer(annotated_example, answers_file)

def get_examples_to_answer(answers_file: str, eval_ds: datasets.Dataset) -> list[dict]:
    print(f"Loading answers from {answers_file}...")
    try:
        done_questions = pd.read_json(answers_file, lines=True)["question"].tolist()
        print(f"Found {len(done_questions)} previous results!")
    except Exception as e:
        print("Error when loading records: ", e)
        print("No usable records! Starting new.")
        done_questions = []
    return [line for line in eval_ds.to_list() if line["question"] not in done_questions and line["file_name"]]

def main():
    args = parse_args()
    print(f"Starting run with arguments: {args}")

    eval_ds = load_gaia_dataset(args.use_raw_dataset, args.set_to_run)
    print("Loaded evaluation dataset:")
    print(pd.DataFrame(eval_ds)["task"].value_counts())

    answers_file = f"output/{args.set_to_run}/{args.run_name}.jsonl"
    tasks_to_run = get_examples_to_answer(answers_file, eval_ds)

    with ThreadPoolExecutor(max_workers=args.concurrency) as exe:
        futures = [
            exe.submit(
                answer_single_question, 
                example, 
                args.model, 
                answers_file, 
                visualizer, 
                args.api_base, 
                args.api_key,
                args.manager_agent_type,
                args.search_agent_type
            )
            for example in tasks_to_run
        ]
        for f in tqdm(as_completed(futures), total=len(tasks_to_run), desc="Processing tasks"):
            f.result()

    # for example in tasks_to_run:
    #     answer_single_question(example, args.model, answers_file, visualizer, args.api_base, args.api_key)
    print("All tasks processed.")

if __name__ == "__main__":
    main()



================================================
FILE: cli/research-agent-cli.py
================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Research Agent CLI - A powerful command-line tool for conducting web research using AI agents.

This script creates a research agent that can search the internet, visit webpages, and analyze content
to answer complex questions. The agent uses a manager-worker architecture with specialized search
and browsing capabilities.

Features:
- Multiple search engines (Google, DuckDuckGo, Wikipedia, etc.)
- Web browsing and content analysis
- PDF and video content inspection
- Multi-step research planning
- Support for various AI models

Examples:
    Basic usage:
        python cli/research-agent-cli.py "What are the latest developments in quantum computing?"
    
    Using a specific model:
        python cli/research-agent-cli.py "Compare Tesla and BYD electric vehicle sales in 2024" --model openai/qwen/qwen3-coder-30b
    
    With custom API endpoint:
        python cli/research-agent-cli.py "Find the population of Tokyo in 2023" --api-base http://localhost:1234/v1 --api-key api-key --model openai/gpt-oss-20b
    
    Using specific search tools:
        python cli/research-agent-cli.py "Latest AI research papers" --search-tools duckduckgo,wikipedia
    
    With different agent types:
        python cli/research-agent-cli.py "Climate change impact on agriculture" --manager-agent-type ToolCallingAgent --search-agent-type CodeAgent
"""


import argparse
import os
import threading

from dotenv import load_dotenv
from huggingface_hub import login

from smolagents import (
    ApiWebSearchTool,
    CodeAgent,
    DuckDuckGoSearchTool,
    GoogleSearchTool,
    LiteLLMModel,
    ToolCallingAgent,
    WebSearchTool,
    WikipediaSearchTool,
)
from src.open_deep_research.text_inspector_tool import TextInspectorTool
from src.open_deep_research.text_web_browser import (
    ArchiveSearchTool,
    FinderTool,
    FindNextTool,
    PageDownTool,
    PageUpTool,
    SimpleTextBrowser,
    VisitTool,
)
from src.open_deep_research.visual_qa import visualizer


load_dotenv(override=True)
login(os.getenv("HF_TOKEN"))

append_answer_lock = threading.Lock()


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "question", type=str, help="for example: 'How many studio albums did Mercedes Sosa release before 2007?'"
    )
    parser.add_argument("--model", '-m', type=str, default="openai/gpt-oss:20b")
    parser.add_argument(
        "--api-base", type=str, help="Base URL for the API endpoint", default="http://localhost:1234/v1"
    )
    parser.add_argument("--api-key", type=str, help="API key for authentication", default="api-key")
    parser.add_argument(
        "--manager-agent-type",
        type=str,
        choices=["CodeAgent", "ToolCallingAgent"],
        default="CodeAgent",
        help="Type of agent to use for the manager (default: CodeAgent)",
    )
    parser.add_argument(
        "--search-agent-type",
        type=str,
        choices=["CodeAgent", "ToolCallingAgent"],
        default="ToolCallingAgent",
        help="Type of agent to use for the search agent (default: ToolCallingAgent)",
    )
    parser.add_argument(
        "--search-tools",
        "-s",
        nargs="+",
        choices=["google", "duckduckgo", "wikipedia", "brave", "websearch"],
        default=["google"],
        help="Search tools to use (default: google). Can specify multiple tools. Options: google (requires SERPAPI/SERPER API key), duckduckgo, wikipedia, brave (requires BRAVE API key), websearch (simple scraper). Example: --search-tools google duckduckgo wikipedia",
    )
    return parser.parse_args()


custom_role_conversions = {"tool-call": "assistant", "tool-response": "user"}

user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"

BROWSER_CONFIG = {
    "viewport_size": 1024 * 5,
    "downloads_folder": "downloads_folder",
    "request_kwargs": {
        "headers": {"User-Agent": user_agent},
        "timeout": 300,
    },
    "serpapi_key": os.getenv("SERPAPI_API_KEY"),
}

os.makedirs(f"./{BROWSER_CONFIG['downloads_folder']}", exist_ok=True)


def create_search_tool(search_tool_type):
    """Create and return a single search tool instance."""
    try:
        if search_tool_type == "google":
            # GoogleSearchTool requires SERPAPI_API_KEY or SERPER_API_KEY
            return GoogleSearchTool(provider="serper")
        elif search_tool_type == "duckduckgo":
            # DuckDuckGoSearchTool requires no API key
            return DuckDuckGoSearchTool()
        elif search_tool_type == "wikipedia":
            # WikipediaSearchTool requires no API key
            return WikipediaSearchTool(user_agent="OpenDeepResearch/1.0 (research@example.com)")
        elif search_tool_type == "brave":
            # ApiWebSearchTool requires BRAVE_API_KEY
            return ApiWebSearchTool()
        elif search_tool_type == "websearch":
            # WebSearchTool requires no API key (uses simple scrapers)
            return WebSearchTool()
        else:
            raise ValueError(f"Unknown search tool type: {search_tool_type}")
    except Exception as e:
        print(f"Error creating search tool '{search_tool_type}': {e}")
        print("Hint: Check that you have the required API key set as an environment variable.")
        if search_tool_type == "google":
            print("For Google search, you need SERPAPI_API_KEY or SERPER_API_KEY")
        elif search_tool_type == "brave":
            print("For Brave search, you need BRAVE_API_KEY")
        raise


def create_search_tools(search_tool_types):
    """Create and return multiple search tool instances."""
    search_tools = []
    failed_tools = []

    # Remove duplicates while preserving order
    unique_tool_types = list(dict.fromkeys(search_tool_types))

    for tool_type in unique_tool_types:
        try:
            tool = create_search_tool(tool_type)
            search_tools.append(tool)
            print(f"✓ Successfully created {tool_type} search tool")
        except Exception as e:
            failed_tools.append((tool_type, str(e)))
            print(f"✗ Failed to create {tool_type} search tool: {e}")

    if not search_tools:
        raise ValueError(f"Failed to create any search tools. Errors: {failed_tools}")

    if failed_tools:
        print(
            f"Warning: {len(failed_tools)} search tool(s) failed to initialize. Continuing with {len(search_tools)} working tool(s)."
        )

    return search_tools


def create_agent(
    model="openai/gpt-oss:20b",
    api_base=None,
    api_key=None,
    manager_agent_type="CodeAgent",
    search_agent_type="ToolCallingAgent",
    search_tools=None,
):
    model_params = {
        "model_id": model,
        "custom_role_conversions": custom_role_conversions,
        "max_completion_tokens": 8192,
    }

    if api_base:
        model_params["api_base"] = api_base
    if api_key:
        model_params["api_key"] = api_key
    if model == "o1":
        model_params["reasoning_effort"] = "high"

    model = LiteLLMModel(**model_params)

    if search_tools is None:
        search_tools = ["google"]

    text_limit = 100000
    browser = SimpleTextBrowser(**BROWSER_CONFIG)
    search_tool_instances = create_search_tools(search_tools)

    # Build web tools list with all search tools
    WEB_TOOLS = []
    WEB_TOOLS.extend(search_tool_instances)  # Add all search tools
    WEB_TOOLS.extend(
        [
            VisitTool(browser),
            PageUpTool(browser),
            PageDownTool(browser),
            FinderTool(browser),
            FindNextTool(browser),
            ArchiveSearchTool(browser),
            TextInspectorTool(model, text_limit),
        ]
    )

    # Create search agent based on specified type
    search_agent_config = {
        "model": model,
        "max_steps": 20,
        "verbosity_level": 2,
        "planning_interval": 4,
        "name": "search_agent",
        "description": """A team member that will search the internet to answer your question.
    Ask him for all your questions that require browsing the web.
    Provide him as much context as possible, in particular if you need to search on a specific timeframe!
    And don't hesitate to provide him with a complex search task, like finding a difference between two webpages.
    Your request must be a real sentence, not a google search! Like "Find me this information (...)" rather than a few keywords.
    """,
        "provide_run_summary": True,
    }

    if search_agent_type == "ToolCallingAgent":
        search_agent_config["tools"] = WEB_TOOLS
        text_webbrowser_agent = ToolCallingAgent(**search_agent_config)
    else:  # CodeAgent
        search_agent_config["tools"] = WEB_TOOLS
        search_agent_config["additional_authorized_imports"] = ["*"]
        text_webbrowser_agent = CodeAgent(**search_agent_config)

    text_webbrowser_agent.prompt_templates["managed_agent"]["task"] += """You can navigate to .txt online files.
    If a non-html page is in another format, especially .pdf or a Youtube video, use tool 'inspect_file_as_text' to inspect it.
    Additionally, if after some searching you find out that you need more information to answer the question, you can use `final_answer` with your request for clarification as argument to request for more information."""

    # Create manager agent based on specified type
    manager_agent_config = {
        "model": model,
        "tools": [visualizer, TextInspectorTool(model, text_limit)],
        "max_steps": 12,
        "verbosity_level": 2,
        "planning_interval": 4,
        "managed_agents": [text_webbrowser_agent],
    }

    if manager_agent_type == "CodeAgent":
        manager_agent_config["additional_authorized_imports"] = ["*"]
        manager_agent = CodeAgent(**manager_agent_config)
    else:  # ToolCallingAgent
        manager_agent = ToolCallingAgent(**manager_agent_config)

    return manager_agent


def main():
    args = parse_args()

    agent = create_agent(
        model=args.model,
        api_base=args.api_base,
        api_key=args.api_key,
        manager_agent_type=args.manager_agent_type,
        search_agent_type=args.search_agent_type,
        search_tools=args.search_tools,
    )

    answer = agent.run(args.question)

    print(f"Got this answer: {answer}")


if __name__ == "__main__":
    main()



================================================
FILE: cli/search-cli.py
================================================
#!/usr/bin/env python
# coding=utf-8

import argparse
import sys
import os
from pathlib import Path

# Add the smolagents source directory to the path
smolagents_path = Path(__file__).parent.parent.parent / "smolagents" / "src"
if smolagents_path.exists():
    sys.path.insert(0, str(smolagents_path))

try:
    from smolagents import (
        DuckDuckGoSearchTool,
        GoogleSearchTool, 
        ApiWebSearchTool,
        WebSearchTool,
        VisitWebpageTool,
        WikipediaSearchTool
    )
except ImportError as e:
    print(f"Error importing smolagents: {e}")
    print("Make sure smolagents is installed and available in your path")
    sys.exit(1)


def create_duckduckgo_tool(args):
    """Create DuckDuckGoSearchTool with specified parameters."""
    kwargs = {}
    if args.max_results:
        kwargs['max_results'] = args.max_results
    if args.rate_limit is not None:
        kwargs['rate_limit'] = args.rate_limit
    return DuckDuckGoSearchTool(**kwargs)


def create_google_tool(args):
    """Create GoogleSearchTool with specified parameters."""
    kwargs = {}
    if args.provider:
        kwargs['provider'] = args.provider
    return GoogleSearchTool(**kwargs)


def create_api_tool(args):
    """Create ApiWebSearchTool with specified parameters."""
    kwargs = {}
    if args.endpoint:
        kwargs['endpoint'] = args.endpoint
    if args.api_key:
        kwargs['api_key'] = args.api_key
    if args.api_key_name:
        kwargs['api_key_name'] = args.api_key_name
    if args.rate_limit is not None:
        kwargs['rate_limit'] = args.rate_limit
    return ApiWebSearchTool(**kwargs)


def create_websearch_tool(args):
    """Create WebSearchTool with specified parameters."""
    kwargs = {}
    if args.max_results:
        kwargs['max_results'] = args.max_results
    if args.engine:
        kwargs['engine'] = args.engine
    return WebSearchTool(**kwargs)


def create_visit_tool(args):
    """Create VisitWebpageTool with specified parameters."""
    kwargs = {}
    if args.max_output_length:
        kwargs['max_output_length'] = args.max_output_length
    return VisitWebpageTool(**kwargs)


def create_wikipedia_tool(args):
    """Create WikipediaSearchTool with specified parameters."""
    kwargs = {}
    if args.user_agent:
        kwargs['user_agent'] = args.user_agent
    if args.language:
        kwargs['language'] = args.language
    if args.content_type:
        kwargs['content_type'] = args.content_type
    if args.extract_format:
        kwargs['extract_format'] = args.extract_format
    return WikipediaSearchTool(**kwargs)


def main():
    parser = argparse.ArgumentParser(
        description="CLI tool to access all smolagents web search tools",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # DuckDuckGo search
  python cli/search-cli.py duckduckgo "Python programming"
  
  # Google search with year filter
  python cli/search-cli.py google "machine learning" --filter-year 2023
  
  # API search (Brave by default)
  python cli/search-cli.py api "AI news"
  
  # Simple web search
  python cli/search-cli.py websearch "climate change" --engine bing
  
  # Visit a webpage
  python cli/search-cli.py visit "https://example.com"
  
  # Wikipedia search
  python cli/search-cli.py wikipedia "Python_(programming_language)" --content-type summary
        """
    )
    
    subparsers = parser.add_subparsers(dest='tool', help='Available search tools')
    
    # DuckDuckGo Search Tool
    ddg_parser = subparsers.add_parser('duckduckgo', help='DuckDuckGo web search')
    ddg_parser.add_argument('query', help='Search query')
    ddg_parser.add_argument('--max-results', type=int, default=10, help='Maximum number of results (default: 10)')
    ddg_parser.add_argument('--rate-limit', type=float, help='Queries per second (default: 1.0, None to disable)')
    
    # Google Search Tool
    google_parser = subparsers.add_parser('google', help='Google search via SerpAPI or Serper')
    google_parser.add_argument('query', help='Search query')
    google_parser.add_argument('--filter-year', type=int, help='Filter results by year')
    google_parser.add_argument('--provider', choices=['serpapi', 'serper'], default='serpapi', help='Search provider (default: serpapi)')
    
    # API Web Search Tool
    api_parser = subparsers.add_parser('api', help='API-based web search (Brave Search by default)')
    api_parser.add_argument('query', help='Search query')
    api_parser.add_argument('--endpoint', help='API endpoint URL')
    api_parser.add_argument('--api-key', help='API key for authentication')
    api_parser.add_argument('--api-key-name', help='Environment variable name for API key')
    api_parser.add_argument('--rate-limit', type=float, help='Queries per second (default: 1.0)')
    
    # Web Search Tool
    web_parser = subparsers.add_parser('websearch', help='Simple web search using HTML/RSS scrapers')
    web_parser.add_argument('query', help='Search query')
    web_parser.add_argument('--max-results', type=int, default=10, help='Maximum number of results (default: 10)')
    web_parser.add_argument('--engine', choices=['duckduckgo', 'bing'], default='duckduckgo', help='Search engine (default: duckduckgo)')
    
    # Visit Webpage Tool
    visit_parser = subparsers.add_parser('visit', help='Visit and read webpage content')
    visit_parser.add_argument('url', help='URL to visit')
    visit_parser.add_argument('--max-output-length', type=int, default=40000, help='Maximum output length (default: 40000)')
    
    # Wikipedia Search Tool
    wiki_parser = subparsers.add_parser('wikipedia', help='Search Wikipedia articles')
    wiki_parser.add_argument('query', help='Wikipedia topic to search')
    wiki_parser.add_argument('--user-agent', default='SearchCLI (smolagents)', help='User agent string')
    wiki_parser.add_argument('--language', default='en', help='Language code (default: en)')
    wiki_parser.add_argument('--content-type', choices=['summary', 'text'], default='text', help='Content type (default: text)')
    wiki_parser.add_argument('--extract-format', choices=['WIKI', 'HTML'], default='WIKI', help='Extract format (default: WIKI)')
    
    args = parser.parse_args()
    
    if not args.tool:
        parser.print_help()
        sys.exit(1)
    
    try:
        # Create the appropriate tool and execute the search
        if args.tool == 'duckduckgo':
            tool = create_duckduckgo_tool(args)
            result = tool.forward(args.query)
            
        elif args.tool == 'google':
            tool = create_google_tool(args)
            if hasattr(args, 'filter_year') and args.filter_year:
                result = tool.forward(args.query, filter_year=args.filter_year)
            else:
                result = tool.forward(args.query)
                
        elif args.tool == 'api':
            tool = create_api_tool(args)
            result = tool.forward(args.query)
            
        elif args.tool == 'websearch':
            tool = create_websearch_tool(args)
            result = tool.forward(args.query)
            
        elif args.tool == 'visit':
            tool = create_visit_tool(args)
            result = tool.forward(args.url)
            
        elif args.tool == 'wikipedia':
            tool = create_wikipedia_tool(args)
            result = tool.forward(args.query)
            
        print(result)
        
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()


================================================
FILE: handbooks/SMOLAGENTS.md
================================================
**Smolagents Handbook**

- Purpose: Complete reference for implementing, extending, and operating projects with the smolagents library. Written for AI assistants and developers.
- Scope: Agents, tools, models, prompts, executors, memory/monitoring, CLI, integrations, and security. Includes code you can copy and adapt.

**At A Glance**
- Core modules: `src/smolagents/{agents.py, tools.py, models.py, local_python_executor.py, default_tools.py, remote_executors.py, memory.py, monitoring.py, utils.py}`
- Agent types: `ToolCallingAgent`, `CodeAgent` (multi‑step ReAct with planning, managed agents, streaming)
- Tools: Class‑based tools (`Tool`), function tools (`@tool`), tool collections (Hub, MCP), wrappers (Spaces, Gradio, LangChain)
- Models: Local (Transformers/MLX/vLLM) and API (OpenAI‑compatible, LiteLLM, HF InferenceClient, Azure OpenAI, Amazon Bedrock) with streaming and tool‑calling
- Executors: Local Python sandbox; remote executors (E2B, Docker, Wasm) for code agents
- Defaults: Web search, visit webpage, Python interpreter, Wikipedia, speech‑to‑text, and more
- Prompts: YAML prompt packs for tool‑calling and code agents with planning templates
- CLI: `smolagent` and `webagent` quick start

**Install**
- Minimal: `pip install -e .` or `pip install smolagents`
- Extras: `pip install -e .[openai,litellm,transformers,mlx-lm,bedrock,e2b,gradio,toolkit,all]`


**Concepts**
- Agent: Orchestrates multi‑step reasoning using a model, tools, and optional managed agents.
- Tool: A callable capability with a name, description, input schema, and output type.
- Model: Chat model adapter that produces `ChatMessage` responses; may support streaming and tool‑calling.
- Executor: Sandbox that executes Python produced by `CodeAgent` (local or remote).
- Memory/Monitoring: Structured steps, token usage, logs, and timing.


**Messages & Types**
- `ChatMessage`: `{role: user|assistant|system|tool-call|tool-response, content: str|list, tool_calls?: list, token_usage?: TokenUsage}`
- Tool‑call schema used by OpenAI‑style providers is produced via `get_tool_json_schema(tool)`.
- Streaming deltas (`ChatMessageStreamDelta`) are aggregated via `agglomerate_stream_deltas`.


**Tools**
- Ways to implement tools:
  - Subclass `Tool` and implement `forward(self, ...)`.
  - Use `@tool` decorator on a typed function with a docstring and Args section.
  - Load from Hub (`Tool.from_hub`, `load_tool`), from an MCP server (`ToolCollection.from_mcp`), from Gradio or LangChain wrappers.
- Tool schema keys:
  - `name: str` unique, Python identifier
  - `description: str` concise, actionable
  - `inputs: dict[str, {type: one of string|boolean|integer|number|image|audio|array|object|any|null, description: str, nullable?: bool}]`
  - `output_type: one of AUTHORIZED_TYPES`

Code you can adapt to create a class‑based tool:

```
from smolagents import Tool

class MySum(Tool):
    name = "sum_numbers"
    description = "Adds two numbers."
    inputs = {
        "a": {"type": "number", "description": "First number"},
        "b": {"type": "number", "description": "Second number"},
    }
    output_type = "number"

    def forward(self, a: float, b: float) -> float:
        return a + b
```

Using the `@tool` decorator (infers schema from type hints + docstring):

```
from smolagents import tool

@tool
def search_docs(query: str, max_results: int = 10) -> str:
    """
    Search internal docs.
    Args:
        query: Text to search.
        max_results: Results to return (1-50).
    """
    # ... implement ...
    return "...markdown summary..."
```

Tool utilities and conversions:
- `Tool.to_code_prompt()` and `Tool.to_tool_calling_prompt()` for model prompting context.
- `Tool.validate_arguments()` auto‑checks schema and `forward` signature.
- `validate_tool_arguments(tool, arguments)` to pre‑validate at runtime.
- `launch_gradio_demo(tool)` to get a simple UI for manual runs.
- Save/share tools:
  - `tool.save(path)` writes `tool.py`, `app.py`, `requirements.txt`.
  - `tool.push_to_hub("org/space-name")` publishes a Gradio Space.
  - Load later with `Tool.from_hub(...)` or high‑level `load_tool(repo_id, ...)`.

Wrappers and collections:
- `Tool.from_space(space_id, name, description, api_name=None, token=None)`: calls a Space endpoint.
- `Tool.from_gradio(gradio_tool)` and `Tool.from_langchain(langchain_tool)`.
- `ToolCollection.from_hub(collection_slug, token=None, trust_remote_code=False)` -> loads all Space tools.
- `ToolCollection.from_mcp(server_parameters, trust_remote_code=True)` -> temporarily connects to an MCP server and adapts exposed tools.


**Default Tools**
- `PythonInterpreterTool` (`python_interpreter`): safe Python evaluator with restricted imports. Input: `code: string`. Output: string (stdout + value). Config: `authorized_imports`.
- `FinalAnswerTool` (`final_answer`): terminates an agent run with the final value. Input `answer: any`. Output: any.
- `UserInputTool` (`user_input`): prompts the console for input. Input `question: string`. Output: string.
- `DuckDuckGoSearchTool` (`web_search`): ddgs API search with rate limiting. Input `query: string`. Output: markdown.
- `GoogleSearchTool` (`web_search`): SERP API or Serper. Inputs: `query: string`, `filter_year?: integer`. Output: markdown. Env: `SERPAPI_API_KEY` or `SERPER_API_KEY`.
- `ApiWebSearchTool` (`web_search`): Brave Search (or custom endpoint) via HTTP. Inputs: `query: string`. Output: markdown. Env: `BRAVE_API_KEY`.
- `WebSearchTool` (`web_search`): simple HTML/RSS scrapers for DuckDuckGo or Bing. Inputs: `query: string`. Output: markdown.
- `VisitWebpageTool` (`visit_webpage`): fetches URL and converts HTML to markdown. Input `url: string`. Output: truncated markdown.
- `WikipediaSearchTool` (`wikipedia_search`): summary or full text. Input `query: string`. Output: markdown. Requires `wikipedia-api` and a user agent string.
- `SpeechToTextTool` (`transcriber`): Whisper pipeline. Input `audio`. Output: string. Requires `transformers`.

Note: `TOOL_MAPPING` adds a base set (`python_interpreter`, `web_search`, `visit_webpage`) when `add_base_tools=True` on an agent (with a special case for `ToolCallingAgent`). All tools can be passed explicitly via `tools=[...]`.


**Agents**
- Both agents extend `MultiStepAgent` and implement a ReAct loop with optional planning and managed agents.

Common capabilities
- Planning: initial and iterative re‑planning via YAML prompts. Controlled by `planning_interval`.
- Managed agents: “team members” usable like tools (each with `name`, `description`, `inputs`, `output_type`).
- Final answer validation: pass `final_answer_checks=[callable]` to gate outputs.
- Logging: rich console, images, timings, token usage; streaming supported if model supports `generate_stream`.

`ToolCallingAgent`
- Produces OpenAI‑style tool calls. The model returns `tool_calls` or plain text parsed into a tool call.
- Creates a combined registry of `tools` and `managed_agents` and passes it to the model via `tools` and `tool_choice`.
- Parallel tool execution when multiple tool calls are returned in a single step.

Quick start:

```
from smolagents import ToolCallingAgent, InferenceClientModel, WebSearchTool

agent = ToolCallingAgent(
    tools=[WebSearchTool()],
    model=InferenceClientModel(model_id="Qwen/Qwen2.5-Coder-32B-Instruct"),
    planning_interval=3,
    stream_outputs=True,
)
print(agent.run("What’s new on Hugging Face this week?"))
```

`CodeAgent`
- The model emits Python “action” code blocks instead of JSON tool calls.
- Code executes in a sandboxed Python executor with authorized imports; code can call tools as functions.
- Executors: `local` (default), `e2b`, `docker`, `wasm` via `executor_type` and `executor_kwargs`.
- Structured internal generation optional (`use_structured_outputs_internally=True`) to enforce “thought + code”.
- Custom code block tags: pass `code_block_tags="markdown"` for triple‑backtick fences, or a `(open, close)` tuple.

Quick start:

```
from smolagents import CodeAgent, InferenceClientModel, WebSearchTool

agent = CodeAgent(
    tools=[WebSearchTool()],
    model=InferenceClientModel(),
    additional_authorized_imports=["numpy"],
    executor_type="local",
    max_steps=10,
)
agent.run("Compute the mean of [1, 2, 3], then search for the result and summarize.")
```

Managed agents inside an agent:

```
researcher = ToolCallingAgent(
    tools=[WebSearchTool()],
    model=InferenceClientModel(),
    name="researcher",
    description="Deep‑dive web researcher",
    provide_run_summary=True,
)
manager = CodeAgent(
    tools=[],
    managed_agents=[researcher],
    model=InferenceClientModel(),
)
manager.run("Draft a one‑page brief on Smolagents with citations.")
```


**Local Python Executor**
- File: `local_python_executor.py`. Executes model code safely with restrictions.
- Built‑ins exposed: curated subset (`BASE_PYTHON_TOOLS`), plus math helpers and safe wrappers.
- Security: disallows dangerous modules/functions, dunder attributes, unapproved builtins; validates imports against an allowlist (`authorized_imports`).
- State: persists variables between steps; captures `print` output under `_print_outputs`.
- Final answer flow: the `final_answer()` tool triggers a dedicated exception to terminate execution cleanly.

Authorize imports:

```
agent = CodeAgent(
  tools=[...],
  model=InferenceClientModel(),
  additional_authorized_imports=["pandas", "numpy", "requests"],
)
```

Remote executors (for isolation or portability):
- `E2BExecutor`: runs in an e2b sandbox (pip extra `e2b`).
- `DockerExecutor`: spins a Jupyter Kernel Gateway inside Docker and executes over websockets.
- `WasmExecutor`: runs Python in Pyodide on Deno with strict permissions.


**Models**
- Base class: `Model` with `generate()` and optional `generate_stream()`.
- Common preparation: `_prepare_completion_kwargs()` flattens/merges messages, stop sequences, response formats, and OpenAI‑style tool specs.
- Structured outputs: only certain providers support `response_format` (see `STRUCTURED_GENERATION_PROVIDERS = ["cerebras", "fireworks-ai"]`).
- Stop sequences behavior: `supports_stop_parameter()` disables `stop` for OpenAI `o3`, `o4-mini`, `gpt-5*` families.

Local backends
- `TransformersModel`: loads `AutoModelForCausalLM` or `AutoModelForImageTextToText` with tokenizers/processors. Supports text and VLMs; stop sequences via custom stopping criteria; streaming with `TextIteratorStreamer`.
- `VLLMModel`: uses `vllm.LLM` and its chat template for fast generation. Supports basic `guided_json` for structured outputs.
- `MLXModel`: Apple Silicon via `mlx-lm` stream API with stop handling.

API/hosted backends
- `OpenAIServerModel` (alias `OpenAIModel`): OpenAI‑compatible APIs, including Fireworks, Groq, etc., by changing `api_base`. Supports streaming and tool‑calling.
- `AzureOpenAIServerModel` (alias `AzureOpenAIModel`): Azure OpenAI deployments using `azure_endpoint`/`api_version`.
- `LiteLLMModel`: calls hundreds of providers through LiteLLM; streaming support; optional `LiteLLMRouterModel` for routing/HA.
- `InferenceClientModel`: Hugging Face Inference Providers (serverless endpoints or base_url). Handles providers list, token, bill_to, and optional structured outputs when provider supports it.
- `AmazonBedrockServerModel`: Bedrock runtime client with role mapping and request shaping; stop via inference config; removes non‑Bedrock fields.

Model selection examples:

```
from smolagents import (
  InferenceClientModel, OpenAIServerModel, LiteLLMModel, TransformersModel, VLLMModel
)

# Hugging Face providers
hf = InferenceClientModel(model_id="Qwen/Qwen2.5-Coder-32B-Instruct", provider="nebius")

# OpenAI‑compatible server (e.g., Fireworks)
fw = OpenAIServerModel(api_key="...", api_base="https://api.fireworks.ai/inference/v1", model_id="accounts/.../models/llama")

# LiteLLM to Groq
groq = LiteLLMModel(model_id="groq/llama3-70b-8192", api_key=os.getenv("GROQ_API_KEY"))

# Local Transformers
local = TransformersModel(model_id="Qwen/Qwen2.5-Coder-7B-Instruct", device_map="auto")

# vLLM serverless local
v = VLLMModel(model_id="meta-llama/Llama-3-8B-Instruct")
```

Streaming usage for compatible backends:

```
events = model.generate_stream(messages, tools_to_call_from=tools)
for delta in events:
    if delta.content:
        ...  # partial text
    if delta.tool_calls:
        ...  # partial tool call struct
```


**Prompts**
- Prompt packs live under `src/smolagents/prompts/`:
  - `toolcalling_agent.yaml`: System prompt + planning templates + final answer top‑off.
  - `code_agent.yaml`: Code‑centric ReAct prompting.
  - `structured_code_agent.yaml`: Enforces `{thought, code}` JSON with `response_format` to stabilize output.
- You can pass `instructions` to inject custom guidance, and tweak planning via `planning_interval`.
- `CodeAgent` supports custom code block tags: set `code_block_tags="markdown"` or a custom `(open, close)`.


**Memory & Monitoring**
- Steps (`ActionStep`, `PlanningStep`, `FinalAnswerStep`) accumulate messages, tool calls, outputs, images/audio, timings, and token usage.
- `AgentLogger` renders markdown, panels, images; `Monitor` composes run stats.
- `RunResult` can be returned with `return_full_result=True`.


**Validation & Safety**
- Tool schema and signature validation at tool init; runtime argument validation via `validate_tool_arguments`.
- Python executor enforces:
  - No dunder attribute access nor unsafe builtins.
  - Denylisted modules/functions (e.g., `os.system`, `subprocess`, `__import__`).
  - Import allowlist; partial wildcard support (`pkg.*`) and `"*"` to allow all (use carefully).
- Remote executors isolate code outside the host process (E2B/Docker/Wasm). Prefer these for untrusted workloads.


**CLI**
- `smolagent`: run a `CodeAgent` from terminal.
  - Example: `smolagent --model-type InferenceClientModel --model-id Qwen/Qwen2.5-Coder-32B-Instruct --tools web_search --imports numpy "What is the pi estimate to 6 decimals?"`
  - Flags: `--model-type`, `--model-id`, `--api-base`, `--api-key`, `--provider`, `--tools`, `--imports`, `--verbosity-level`.
- `webagent`: drive a real browser with Helium + CodeAgent (see `vision_web_browser.py`). Requires a vision‑capable model for complex tasks.


**Integration Patterns**
- Use Spaces as tools for composability; publish internal capabilities as Spaces and load via `Tool.from_space`.
- Use MCP servers to surface org systems as tools dynamically (`ToolCollection.from_mcp`).
- Mix models (LiteLLM Router) for resiliency and cost/perf tradeoffs.
- Build gradio demos for each tool to test UX and I/O before agent integration.


**Reference: Essential APIs**
- Tool base and decorator (extracts from `tools.py`):

```
class Tool(BaseTool):
    name: str
    description: str
    inputs: dict[str, dict[str, str | type | bool]]
    output_type: str
    def forward(self, *args, **kwargs): ...

def tool(tool_function: Callable) -> Tool:  # turns a typed function into a Tool
    ...  # infers name/description/inputs/return from docstring/type hints
```

- Agent types (extracts from `agents.py`):

```
class MultiStepAgent:
    def run(self, task: str, images: list[PIL.Image.Image] | None = None, return_full_result: bool | None = None): ...
    def step(self, memory_step: ActionStep) -> Any: ...

class ToolCallingAgent(MultiStepAgent):
    # uses model tool-calling and parallel tool execution
    ...

class CodeAgent(MultiStepAgent):
    def __init__(..., executor_type: Literal["local","e2b","docker","wasm"] = "local", ...): ...
    def create_python_executor(self) -> PythonExecutor: ...
```

- Models (extracts from `models.py`):

```
class Model:
    def generate(self, messages: list[ChatMessage], *, stop_sequences=None, response_format=None, tools_to_call_from=None, **kwargs) -> ChatMessage: ...
    def generate_stream(self, ...) -> Generator[ChatMessageStreamDelta]: ...  # optional

class TransformersModel(Model): ...
class VLLMModel(Model): ...
class MLXModel(Model): ...
class OpenAIServerModel(Model): ...
class AzureOpenAIServerModel(OpenAIServerModel): ...
class LiteLLMModel(Model): ...
class LiteLLMRouterModel(LiteLLMModel): ...
class InferenceClientModel(Model): ...
class AmazonBedrockServerModel(Model): ...
```

- Executors (extracts from `local_python_executor.py` and `remote_executors.py`):

```
class PythonExecutor(ABC):
    def send_tools(self, tools: dict[str, Tool]) -> None: ...
    def send_variables(self, variables: dict[str, Any]) -> None: ...
    def __call__(self, code_action: str) -> CodeOutput: ...

class LocalPythonExecutor(PythonExecutor): ...
class E2BExecutor(PythonExecutor): ...
class DockerExecutor(PythonExecutor): ...
class WasmExecutor(PythonExecutor): ...
```


**Patterns & Tips**
- Keep tool descriptions concrete and short; list arguments and return semantics.
- Prefer returning markdown from tools for readable logs and downstream LLM interpretation.
- Use `planning_interval` to re‑plan on long tasks; stream outputs during planning and acting to tighten feedback loops.
- Validate tool arguments before executing costly actions.
- For CodeAgent, start with `local` executor in trusted contexts; adopt `wasm` or `e2b` for untrusted code.
- For OpenAI‑style servers that do not support `stop`, rely on prompt templates and natural termination markers.
- Use `response_format` for structured internal generation when supported to reduce parsing errors.


**Troubleshooting**
- “No tool call was found”: the model returned plain text without a tool call; use `parse_tool_calls` or adjust prompts.
- “Forbidden function evaluation”: CodeAgent attempted to call disallowed builtins; include it in authorized imports or wrap in a tool.
- “No results found” in search tools: loosen query or adjust year filter.
- Docker executor startup issues: confirm Docker is running and port availability; inspect logs for kernel gateway errors.
- Bedrock: ensure required boto3 version and credentials (or API key) are present.


**CLI Recipes**
- Fireworks via OpenAI client: `smolagent --model-type OpenAIServerModel --api-base https://api.fireworks.ai/inference/v1 --api-key $FIREWORKS_API_KEY --model-id accounts/fireworks/models/llama-v3p1-70b-instruct`.
- LiteLLM Groq: `smolagent --model-type LiteLLMModel --model-id groq/llama-3.1-70b --api-key $GROQ_API_KEY`.
- Local Transformers: `smolagent --model-type TransformersModel --model-id Qwen/Qwen2.5-Coder-7B-Instruct --tools web_search`.


**Security Notes**
- Never hardcode secrets; prefer environment variables (`python-dotenv` supported).
- Review third‑party tool code before loading with `Tool.from_hub` or MCP; pass `trust_remote_code=True` only when appropriate.
- When exposing broad imports (`"*"`), ensure environment packages are present and acceptable for your threat model.


**Repo Development**
- Create env: `python -m venv .venv && source .venv/bin/activate && pip install -e .[dev]`
- Lint: `make quality`; Auto‑fix: `make style`; Tests: `make test`
- CLI quick check: `smolagent --help`, `webagent --help`


**Appendix: Full Default Tools Map**
- Available class names and canonical names:
  - `PythonInterpreterTool` -> `python_interpreter`
  - `FinalAnswerTool` -> `final_answer`
  - `UserInputTool` -> `user_input`
  - `DuckDuckGoSearchTool` -> `web_search`
  - `GoogleSearchTool` -> `web_search`
  - `ApiWebSearchTool` -> `web_search`
  - `WebSearchTool` -> `web_search`
  - `VisitWebpageTool` -> `visit_webpage`
  - `WikipediaSearchTool` -> `wikipedia_search`
  - `SpeechToTextTool` -> `transcriber`

For up‑to‑date signatures and behavior, see `src/smolagents/default_tools.py`.


**Appendix: Prompt Keys**
- `toolcalling_agent.yaml` keys: `system_prompt`, `planning.initial_plan`, `planning.update_plan_pre_messages`, `planning.update_plan_post_messages`, `managed_agent.task`, `managed_agent.report`, `final_answer.pre_messages`, `final_answer.post_messages`.
- `code_agent.yaml` and `structured_code_agent.yaml` follow similar structure tailored for code outputs.


That’s the core of smolagents. With these APIs and patterns, assistants can implement tools, assemble agents, choose backends, and run safely in a variety of environments.




================================================
FILE: src/open_deep_research/__init__.py
================================================
# -*- coding: utf-8 -*-
"""
Open Deep Research - An open replication of OpenAI's Deep Research.

This package provides a multi-agent architecture for conducting comprehensive web research,
achieving 55% pass@1 on the GAIA validation set. The system uses specialized agents for
search, web browsing, and document analysis.

Key Components:
- Manager Agent: Orchestrates research process and handles complex reasoning
- Search Agent: Specialized for web browsing and information gathering  
- Text Browser: Advanced web scraping and navigation tools
- Document Processing: Handles PDFs, Excel, images, and other file formats
- Visual QA: Image analysis and visual document processing
"""

__version__ = "0.1.0"
__author__ = "Pasquale Minervini"
__email__ = "p.minervini@gmail.com"

from open_deep_research.text_web_browser import SimpleTextBrowser
from open_deep_research.text_inspector_tool import TextInspectorTool
from open_deep_research.visual_qa import visualizer

__all__ = [
    "SimpleTextBrowser", 
    "TextInspectorTool",
    "visualizer",
]



================================================
FILE: src/open_deep_research/cookies.py
================================================
# -*- coding: utf-8 -*-

from requests.cookies import RequestsCookieJar

COOKIES_LIST = [
    {
        "domain": ".youtube.com",
        "expirationDate": 1718884961,
        "hostOnly": False,
        "httpOnly": False,
        "name": "ST-xuwub9",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "session_logininfo=AFmmF2swRAIgf4gadACOuWOcipI1anW-dakEjtidNLkufnOC8uml7EECIDh2YisqWELDBJPTGUysCucJ3I0wjXxYjVHro1LHrdW0%3AQUQ3MjNmd2Jiajl3OWZYRnpFNnZlWWV5ZGJWZ0hpcmp4LVVPU280bk4zOS03Z0ozZG9fOFhWZ0dXaVo3NG1wTEg1b3hGaG10TFBlaFBnTlJfbER5bEp0aFhoNS1OLVhYNFRZT2F6ajgzOFpDbGhlUjZpMWRETlFFRjFfTTRiM0RnNTROSkdmMTFMVjFic1VuZ2trbGp4aktDa0JJUC1BWDh3",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753004444.745411,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-YEC",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "CgtRVnI5LW1zRHlQVSjbtNCzBjIhCgJGUhIbEhcSFRMLFBUWFwwYGRobHB0eHw4PIBAREiAk",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050824,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSID",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBB4ezJ_bdWu46a7YwObVn44wACgYKAakSARQSFQHGX2MicJcTzecTKH6bHzqU6TMbTxoVAUF8yKqQYK-MoI6Ql3vI2oYTB3E-0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974642,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SIDCC",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "AKEyXzWQZauHKOo8t87zoEcjaVNIYUX54ohoWXT-tX4aAhEuZzIIptxZAcNkHuG2oDXYL6t-lw",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050652,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBB6VHrZcC3gBAsFPbCQ0gF5AACgYKAYkSARQSFQHGX2Mi9kt0gHg5CxCYSkLQGHWaeBoVAUF8yKre_V6r3jZVak6JV4o2Q0FL0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420958.397534,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSIDTS",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "sidts-CjIB3EgAEkYL2L-GfrEzW5Dfy62S9oefGNLgst78S_986htCnGcfkxECch_9oz-qytSsZBAA",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753433494.44729,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_M0180HEFCY",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718871908.1.0.1718873494.0.0.0",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050933,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SAPISID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974764,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSIDCC",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AKEyXzWHDSoXGCZpZhPxRrnC7B1s8zGIUjeMVyvgtQfsm1fs92lXPtFEI_td9LBUyqVUe0xK",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050881,
        "hostOnly": False,
        "httpOnly": True,
        "name": "SSID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AmlwXHnQvOQ10LVd-",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050959,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__Secure-1PAPISID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050795,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBBrlk7lRpKQGywAHEon7WGQAACgYKAQsSARQSFQHGX2MirAmnSRdZl6GPG6KLd4hOihoVAUF8yKoV17Tcj1a_OenIOkf2wBjO0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050993,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__Secure-3PAPISID",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974815,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSIDCC",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AKEyXzXM5UjKUEXwSHVmRAIo6hGHA4G63adj3EE1VdNriD0f38jZQbsUKiD4LQbA3BValmTFDg",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420958.397647,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSIDTS",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "sidts-CjIB3EgAEkYL2L-GfrEzW5Dfy62S9oefGNLgst78S_986htCnGcfkxECch_9oz-qytSsZBAA",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050908,
        "hostOnly": False,
        "httpOnly": False,
        "name": "APISID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "IlQWLPjdNqziwCrV/ANG7Z4x5FF-IBxbZk",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050855,
        "hostOnly": False,
        "httpOnly": True,
        "name": "HSID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "AasA7hmRuTFv7vjoq",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753435873.577793,
        "hostOnly": False,
        "httpOnly": True,
        "name": "LOGIN_INFO",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AFmmF2swRAIgf4gadACOuWOcipI1anW-dakEjtidNLkufnOC8uml7EECIDh2YisqWELDBJPTGUysCucJ3I0wjXxYjVHro1LHrdW0:QUQ3MjNmd2Jiajl3OWZYRnpFNnZlWWV5ZGJWZ0hpcmp4LVVPU280bk4zOS03Z0ozZG9fOFhWZ0dXaVo3NG1wTEg1b3hGaG10TFBlaFBnTlJfbER5bEp0aFhoNS1OLVhYNFRZT2F6ajgzOFpDbGhlUjZpMWRETlFFRjFfTTRiM0RnNTROSkdmMTFMVjFic1VuZ2trbGp4aktDa0JJUC1BWDh3",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753444956.555608,
        "hostOnly": False,
        "httpOnly": False,
        "name": "PREF",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "f4=4000000&f6=40000000&tz=Europe.Paris&f5=30000&f7=100",
    },
]

COOKIES_LIST += [
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "isInstIp",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "False",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1734423981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__eoi",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "ID=c26f752377373146:T=1718871981:RT=1718884914:S=AA-AfjZw-T_OOX2kW2LLaFzXImgc",
    },
    {
        "domain": ".www.researchgate.net",
        "expirationDate": 1753444909.646103,
        "hostOnly": False,
        "httpOnly": True,
        "name": "ptc",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "RG1.8947708639250500550.1718872043",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750507578,
        "hostOnly": False,
        "httpOnly": False,
        "name": "euconsent-v2-didomi",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "CQAgmoAQAgmoAAHABBENA5EsAP_gAEPgAAYgJ2pB5G5UTWlBIG53YMskIAUFhFBoQEAgAACAAwIBSBIAIIwEAGAAIAgAICACAAIAIBIAIABAGAAAAAAAYIAAIAAIAAAQIAAKIAAAAAAAAgBQAAgIAgggEAAAgEBEABAAgAAAEIIAQNgACgAAACCAAAAAAAABAAAAAAAAQAAAAAAAYCQAAAJIAAAAACAIABAIAAAAAAAAAAAAAAAABBAAIJ2wPIAFAAXABQAFQALgAcAA8ACAAEgALwAZAA0ACIAEcAJgAUgAqgBcADEAGgAPQAfgBEACOAE4AMMAZYA0QBsgDkAHOAO4AfsBBwEIAItARwBHQC6gHUAO2Ae0A_4CHQEXgJ2AUOAo8BT4CpQFqALYAXmAwQBkgDLAGXANjAhCBG8CbAE3gJ1gTtAA.f_wACHwAAAAA",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718885236,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gat",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
    {
        "domain": "www.researchgate.net",
        "expirationDate": 1721477183,
        "hostOnly": True,
        "httpOnly": False,
        "name": "_pbjs_userid_consent_data",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "3524755945110770",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1752567981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__gads",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "ID=eca2adb88969c830:T=1718871981:RT=1718884914:S=ALNI_MY2qZchynrhWX6hWMlaI87Pcj9riQ",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718886709.646173,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__cf_bm",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "IkQ_J4ciBzKQduRvjqsfSmQu8UygDWbHeROO5JVccfo-1718884909-1.0.1.1-qvNGEdbfI0HfhFP6kwe7R7mkTqODNhFuKhs72lLly6K2BOPMG3kbahpQFGvPK0U8FUfkznkq65gngd1sWj7sDA",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1752567981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__gpi",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "UID=00000e4e9aa2e6f2:T=1718871981:RT=1718884914:S=ALNI_MYFNrgzkKn7K6Bd2y8hC6GJCvDiSg",
    },
    {
        "domain": ".researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "_cfuvid",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "_GPmGZkBymiH3UiqTqzakEpi98br3nfFUWC2_u_wqkc-1718884909785-0.0.1.1-604800000",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1753445177.271667,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.1.1525244793.1718885177",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1753445177.271482,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_4P31SJ70EJ",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718885177.1.0.1718885177.0.0.0",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718971576,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gid",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.854907463.1718885177",
    },
    {
        "domain": ".www.researchgate.net",
        "expirationDate": 1750407982.506505,
        "hostOnly": False,
        "httpOnly": True,
        "name": "did",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "1dWLO3C6am8l667Q4VUlBo0O1LI49Qi2Vw21SJEXHavBDYT56DI9007W5rYGVFVH",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750507578,
        "hostOnly": False,
        "httpOnly": False,
        "name": "didomi_token",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "eyJ1c2VyX2lkIjoiMTkwMzU4YTUtNWU2My02Y2UzLWJlNzAtZGFjNzVmYjdiY2ExIiwiY3JlYXRlZCI6IjIwMjQtMDYtMjBUMTI6MDY6MTYuODA2WiIsInVwZGF0ZWQiOiIyMDI0LTA2LTIwVDEyOjA2OjE4Ljc4MVoiLCJ2ZW5kb3JzIjp7ImVuYWJsZWQiOlsidHdpdHRlciIsImdvb2dsZSIsImM6bGlua2VkaW4tbWFya2V0aW5nLXNvbHV0aW9ucyIsImM6b3duZXJpcSIsImM6b21uaXR1cmUtYWRvYmUtYW5hbHl0aWNzIiwiYzp0ZWNobm9yYXRpLW1lZGlhIiwiYzppbnRlcmNvbSIsImM6aW50ZW50LWlxIiwiYzppcHJvbSIsImM6bGlua2VkaW4iLCJjOmFtYXpvbmFkdi16Y1hGTEI2WCIsImM6bWVkaWFuZXQtY1V3YUtFNnoiLCJjOmluZGV4ZXhjaC1OWkNRTTY4UCIsImM6emVvdGFwZ21iLWQ3YndtdGp3IiwiYzp0cmlwbGVsaWYtZGRKSDM0clkiLCJjOnJ0YmhvdXNlLWI4Y2RIOHRNIiwiYzptZHByaW1pcy1lYU4yOVdjUCIsImM6bG9vcG1lbGktVGRhWXRCUHEiLCJjOm1hZ25pdGVpbi05d1RZTHFSRCIsImM6Ymlkc3dpdGNoLWQ2N0V3N1c5IiwiYzpvcmFjbGVhZHYtcUhlREptQUwiLCJjOmdvb2dsZWFuYS00VFhuSmlnUiIsImM6bG90YW1lc29sLURIaTdMUmpNIiwiYzpuZXh0bWlsbGUtR0pyZlg4VWMiLCJjOm5yaWNodGVjLXFVVlEyUlFxIiwiYzpicml0ZXBvb2wtQldWeVdHeVUiLCJjOnRhcGFkaW5jLXFxY2tVN1BXIiwiYzppZDV0ZWNobi16Tk1KNGR3ZiIsImM6bWljcm9zb2Z0IiwiYzpwZXJtdXRpdmUtSjdpaHJlTWsiLCJjOm9wZXJhc29mdC1CY1hjRFZKTSIsImM6cG9zdGhvZy1Cakp4RmRGOSJdfSwicHVycG9zZXMiOnsiZW5hYmxlZCI6WyJnZW9sb2NhdGlvbl9kYXRhIiwiZGV2aWNlX2NoYXJhY3RlcmlzdGljcyJdfSwidmVuZG9yc19saSI6eyJlbmFibGVkIjpbImdvb2dsZSIsImM6b3BlcmFzb2Z0LUJjWGNEVkpNIl19LCJ2ZXJzaW9uIjoyLCJhYyI6IkRIU0FvQUZrQWNnQTVnSHFnUUhBeGdCNndEMTRJR0FRTkFqMEJJd0NTY0VyQUtCd1YtZ3MxQmgwREc0R09nQUEuREhTQW9BRmtBY2dBNWdIcWdRSEF4Z0I2d0QxNElHQVFOQWowQkl3Q1NjRXJBS0J3Vi1nczFCaDBERzRHT2dBQSJ9",
    },
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "hasPdpNext",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "False",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750421183,
        "hostOnly": False,
        "httpOnly": False,
        "name": "ph_phc_ma1XTQyee96N1GML6qUTgLQRiDifnRcE9STiHTZ0CfZ_posthog",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "%7B%22distinct_id%22%3A%220190358a-56a1-7313-83b0-d13dddeac787%22%2C%22%24sesid%22%3A%5B1718885183223%2C%220190358a-56a1-7313-83b0-d13b2b87778d%22%2C1718885176993%5D%2C%22%24session_is_sampled%22%3Atrue%7D",
    },
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "sid",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "qmH5Lc4f0CUJ3zeaxORcV0S8I8V1MuCFZtcIQqPYtv1XPejrbSLAQRbT50PL40TqeKQ1XsQDWt9gtYVzuL80bRmPjw6jn3cQ0ikNqW40maHcQ3JL2Vfa8ZZf0j7p35eJ",
    },
]

COOKIES_LIST += [
    {
        "domain": "github.com",
        "hostOnly": True,
        "httpOnly": True,
        "name": "_gh_sess",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "P%2Fmof1avuqwHaUQUIJR%2FZYn7jqbT7lgGuTGjp1BGAFIG5UpNDusEE3b8dRjz0eATE5xPdPjLYFqMs%2FI9AOalKX4YuYfSEEnxCMawU01099b4o9Xzzcv%2BmecrmO0Q8q%2Bdq1h8SIv6nvPP7HzlFesl8ysafb9b%2F0q6dTArKdSOurasza8UgLSYD08ofA50Pcm0IG7CTzF8ZCizrGgGTMi%2F%2B7L3E17jav5PM1Sf2vQKg15Gbg1QIOppJJHzlufgQoZigqFv%2BWznaws0Tt7Y2lSFCw%3D%3D--CJRhqMXJnwOaJgk4--DhUErlL4GdROikEjKD4O9g%3D%3D",
    },
    {
        "domain": ".github.com",
        "expirationDate": 1750408875.763785,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_octo",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "GH1.1.728652011.1718872875",
    },
    {
        "domain": ".github.com",
        "expirationDate": 1750408875.763926,
        "hostOnly": False,
        "httpOnly": True,
        "name": "logged_in",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "no",
    },
    {
        "domain": ".github.com",
        "hostOnly": False,
        "httpOnly": False,
        "name": "preferred_color_mode",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "dark",
    },
    {
        "domain": ".github.com",
        "hostOnly": False,
        "httpOnly": False,
        "name": "tz",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "Europe%2FParis",
    },
]

COOKIES_LIST += [
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718886430,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gat",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718972770,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gid",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.402246368.1606169825",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1753446370.315621,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.1301409987.1606169825",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1750422367,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_hjid",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "07f80263-a631-4bf4-8ffd-8fc8912085e2",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718888167,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_hjFirstSeen",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
]
COOKIES_LIST += [
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "AWSELBCORS",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "CBD1D7FF1216388FA48838CBCA4774FD22800B8FB548A40EF92BB0994D5B77A8410307CDEAA69C52236663F2BF89B252C17BC0FCDF790FD59771BDDF6EA8CA4CFD29D8733F",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1753452454.637671,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_9R61FWK9H5",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718892454.1.0.1718892454.0.0.0",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1753452454.63421,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.1.2021310691.1718892455",
    },
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "AWSELB",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": True,
        "storeId": None,
        "value": "CBD1D7FF1216388FA48838CBCA4774FD22800B8FB548A40EF92BB0994D5B77A8410307CDEAA69C52236663F2BF89B252C17BC0FCDF790FD59771BDDF6EA8CA4CFD29D8733F",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1750428454,
        "hostOnly": False,
        "httpOnly": False,
        "name": "OptanonAlertBoxClosed",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "2024-06-20T14:07:34.583Z",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1750428454,
        "hostOnly": False,
        "httpOnly": False,
        "name": "OptanonConsent",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "isGpcEnabled=0&datestamp=Thu+Jun+20+2024+16%3A07%3A34+GMT%2B0200+(heure+d%E2%80%99%C3%A9t%C3%A9+d%E2%80%99Europe+centrale)&version=202310.2.0&browserGpcFlag=0&isIABGlobal=False&hosts=&landingPath=NotLandingPage&groups=C0001%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1",
    },
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "XSRF-TOKEN",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "6957be7a-bcb4-4d59-a522-ea9b6b210ed9",
    },
]

# Create a RequestsCookieJar instance
COOKIES = RequestsCookieJar()

# Add cookies to the jar
for cookie in COOKIES_LIST:
    COOKIES.set(cookie["name"], cookie["value"], domain=cookie["domain"], path=cookie["path"])



================================================
FILE: src/open_deep_research/mdconvert.py
================================================
# -*- coding: utf-8 -*-

# This is copied from Magentic-one's great repo: https://github.com/microsoft/autogen/blob/v0.4.4/python/packages/autogen-magentic-one/src/autogen_magentic_one/markdown_browser/mdconvert.py
# Thanks to Microsoft researchers for open-sourcing this!
# type: ignore
import base64
import copy
import html
import json
import mimetypes
import os
import re
import shutil
import subprocess
import sys
import tempfile
import traceback
import zipfile
from typing import Any
from urllib.parse import parse_qs, quote, unquote, urlparse, urlunparse

import mammoth
import markdownify
import pandas as pd
import pdfminer
try:
    import pdfminer.high_level
    pdfminer_extract_text = pdfminer.high_level.extract_text
except ImportError:
    # Fallback for newer pdfminer versions that have HOCRConverter import issues
    # Implement basic text extraction using lower-level pdfminer functions
    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
    from pdfminer.converter import TextConverter
    from pdfminer.layout import LAParams
    from pdfminer.pdfpage import PDFPage
    from io import StringIO
    
    def pdfminer_extract_text(path):
        """Fallback text extraction for pdfminer compatibility issues"""
        output = StringIO()
        with open(path, 'rb') as file:
            rsrcmgr = PDFResourceManager()
            device = TextConverter(rsrcmgr, output, laparams=LAParams())
            interpreter = PDFPageInterpreter(rsrcmgr, device)
            
            for page in PDFPage.get_pages(file):
                interpreter.process_page(page)
            
            device.close()
        
        return output.getvalue()
import pptx

# File-format detection
import puremagic
import pydub
import requests
import speech_recognition as sr
from bs4 import BeautifulSoup
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import SRTFormatter

class _CustomMarkdownify(markdownify.MarkdownConverter):
    """
    A custom version of markdownify's MarkdownConverter. Changes include:

    - Altering the default heading style to use '#', '##', etc.
    - Removing javascript hyperlinks.
    - Truncating images with large data:uri sources.
    - Ensuring URIs are properly escaped, and do not conflict with Markdown syntax
    """

    def __init__(self, **options: Any):
        options["heading_style"] = options.get("heading_style", markdownify.ATX)
        # Explicitly cast options to the expected type if necessary
        super().__init__(**options)

    def convert_hn(self, n: int, el: Any, text: str, convert_as_inline: bool) -> str:
        """Same as usual, but be sure to start with a new line"""
        if not convert_as_inline:
            if not re.search(r"^\n", text):
                return "\n" + super().convert_hn(n, el, text, convert_as_inline)  # type: ignore

        return super().convert_hn(n, el, text, convert_as_inline)  # type: ignore

    def convert_a(self, el: Any, text: str, convert_as_inline: bool):
        """Same as usual converter, but removes Javascript links and escapes URIs."""
        prefix, suffix, text = markdownify.chomp(text)  # type: ignore
        if not text:
            return ""
        href = el.get("href")
        title = el.get("title")

        # Escape URIs and skip non-http or file schemes
        if href:
            try:
                parsed_url = urlparse(href)  # type: ignore
                if parsed_url.scheme and parsed_url.scheme.lower() not in ["http", "https", "file"]:  # type: ignore
                    return "%s%s%s" % (prefix, text, suffix)
                href = urlunparse(parsed_url._replace(path=quote(unquote(parsed_url.path))))  # type: ignore
            except ValueError:  # It's not clear if this ever gets thrown
                return "%s%s%s" % (prefix, text, suffix)

        # For the replacement see #29: text nodes underscores are escaped
        if (
            self.options["autolinks"]
            and text.replace(r"\_", "_") == href
            and not title
            and not self.options["default_title"]
        ):
            # Shortcut syntax
            return "<%s>" % href
        if self.options["default_title"] and not title:
            title = href
        title_part = ' "%s"' % title.replace('"', r"\"") if title else ""
        return "%s[%s](%s%s)%s" % (prefix, text, href, title_part, suffix) if href else text

    def convert_img(self, el: Any, text: str, convert_as_inline: bool) -> str:
        """Same as usual converter, but removes data URIs"""

        alt = el.attrs.get("alt", None) or ""
        src = el.attrs.get("src", None) or ""
        title = el.attrs.get("title", None) or ""
        title_part = ' "%s"' % title.replace('"', r"\"") if title else ""
        if convert_as_inline and el.parent.name not in self.options["keep_inline_images_in"]:
            return alt

        # Remove dataURIs
        if src.startswith("data:"):
            src = src.split(",")[0] + "..."

        return "![%s](%s%s)" % (alt, src, title_part)

    def convert_soup(self, soup: Any) -> str:
        return super().convert_soup(soup)  # type: ignore

class DocumentConverterResult:
    """The result of converting a document to text."""

    def __init__(self, title: str | None = None, text_content: str = ""):
        self.title: str | None = title
        self.text_content: str = text_content

class DocumentConverter:
    """Abstract superclass of all DocumentConverters."""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        raise NotImplementedError()

class PlainTextConverter(DocumentConverter):
    """Anything with content type text/plain"""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Guess the content type from any file extension that might be around
        content_type, _ = mimetypes.guess_type("__placeholder" + kwargs.get("file_extension", ""))

        # Only accept text files
        if content_type is None:
            return None
        # elif "text/" not in content_type.lower():
        #     return None

        text_content = ""
        with open(local_path, "rt", encoding="utf-8") as fh:
            text_content = fh.read()
        return DocumentConverterResult(title=None, text_content=text_content)

class HtmlConverter(DocumentConverter):
    """Anything with content type text/html"""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Bail if not html
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None

        result = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            result = self._convert(fh.read())

        return result

    def _convert(self, html_content: str) -> None | DocumentConverterResult:
        """Helper function that converts and HTML string."""

        # Parse the string
        soup = BeautifulSoup(html_content, "html.parser")

        # Remove javascript and style blocks
        for script in soup(["script", "style"]):
            script.extract()

        # Print only the main content
        body_elm = soup.find("body")
        webpage_text = ""
        if body_elm:
            webpage_text = _CustomMarkdownify().convert_soup(body_elm)
        else:
            webpage_text = _CustomMarkdownify().convert_soup(soup)

        assert isinstance(webpage_text, str)

        return DocumentConverterResult(title=None if soup.title is None else soup.title.string, text_content=webpage_text)

class WikipediaConverter(DocumentConverter):
    """Handle Wikipedia pages separately, focusing only on the main document content."""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Bail if not Wikipedia
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None
        url = kwargs.get("url", "")
        if not re.search(r"^https?:\/\/[a-zA-Z]{2,3}\.wikipedia.org\/", url):
            return None

        # Parse the file
        soup = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            soup = BeautifulSoup(fh.read(), "html.parser")

        # Remove javascript and style blocks
        for script in soup(["script", "style"]):
            script.extract()

        # Print only the main content
        body_elm = soup.find("div", {"id": "mw-content-text"})
        title_elm = soup.find("span", {"class": "mw-page-title-main"})

        webpage_text = ""
        main_title = None if soup.title is None else soup.title.string

        if body_elm:
            # What's the title
            if title_elm and len(title_elm) > 0:
                main_title = title_elm.string  # type: ignore
                assert isinstance(main_title, str)

            # Convert the page
            webpage_text = f"# {main_title}\n\n" + _CustomMarkdownify().convert_soup(body_elm)
        else:
            webpage_text = _CustomMarkdownify().convert_soup(soup)

        return DocumentConverterResult(title=main_title, text_content=webpage_text)

class YouTubeConverter(DocumentConverter):
    """Handle YouTube specially, focusing on the video title, description, and transcript."""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Bail if not YouTube
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None
        url = kwargs.get("url", "")
        if not url.startswith("https://www.youtube.com/watch?"):
            return None

        # Parse the file
        soup = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            soup = BeautifulSoup(fh.read(), "html.parser")

        # Read the meta tags
        assert soup.title is not None and soup.title.string is not None
        metadata: dict[str, str] = {"title": soup.title.string}
        for meta in soup(["meta"]):
            for a in meta.attrs:
                if a in ["itemprop", "property", "name"]:
                    metadata[meta[a]] = meta.get("content", "")
                    break

        # We can also try to read the full description. This is more prone to breaking, since it reaches into the page implementation
        try:
            for script in soup(["script"]):
                content = script.text
                if "ytInitialData" in content:
                    lines = re.split(r"\r?\n", content)
                    obj_start = lines[0].find("{")
                    obj_end = lines[0].rfind("}")
                    if obj_start >= 0 and obj_end >= 0:
                        data = json.loads(lines[0][obj_start : obj_end + 1])
                        attrdesc = self._findKey(data, "attributedDescriptionBodyText")  # type: ignore
                        if attrdesc:
                            metadata["description"] = str(attrdesc["content"])
                    break
        except Exception:
            pass

        # Start preparing the page
        webpage_text = "# YouTube\n"

        title = self._get(metadata, ["title", "og:title", "name"])  # type: ignore
        assert isinstance(title, str)

        if title:
            webpage_text += f"\n## {title}\n"

        stats = ""
        views = self._get(metadata, ["interactionCount"])  # type: ignore
        if views:
            stats += f"- **Views:** {views}\n"

        keywords = self._get(metadata, ["keywords"])  # type: ignore
        if keywords:
            stats += f"- **Keywords:** {keywords}\n"

        runtime = self._get(metadata, ["duration"])  # type: ignore
        if runtime:
            stats += f"- **Runtime:** {runtime}\n"

        if len(stats) > 0:
            webpage_text += f"\n### Video Metadata\n{stats}\n"

        description = self._get(metadata, ["description", "og:description"])  # type: ignore
        if description:
            webpage_text += f"\n### Description\n{description}\n"

        transcript_text = ""
        parsed_url = urlparse(url)  # type: ignore
        params = parse_qs(parsed_url.query)  # type: ignore
        if "v" in params:
            assert isinstance(params["v"][0], str)
            video_id = str(params["v"][0])
            try:
                # Must be a single transcript.
                transcript = YouTubeTranscriptApi.get_transcript(video_id)  # type: ignore
                # transcript_text = " ".join([part["text"] for part in transcript])  # type: ignore
                # Alternative formatting:
                transcript_text = SRTFormatter().format_transcript(transcript)
            except Exception:
                pass
        if transcript_text:
            webpage_text += f"\n### Transcript\n{transcript_text}\n"

        title = title if title else soup.title.string
        assert isinstance(title, str)

        return DocumentConverterResult(title=title, text_content=webpage_text)

    def _get(self, metadata: dict[str, str], keys: list[str], default: str | None = None) -> str | None:
        for k in keys:
            if k in metadata:
                return metadata[k]
        return default

    def _findKey(self, json: Any, key: str) -> str | None:  # TODO: Fix json type
        if isinstance(json, list):
            for elm in json:
                ret = self._findKey(elm, key)
                if ret is not None:
                    return ret
        elif isinstance(json, dict):
            for k in json:
                if k == key:
                    return json[k]
                else:
                    ret = self._findKey(json[k], key)
                    if ret is not None:
                        return ret
        return None

class PdfConverter(DocumentConverter):
    """
    Converts PDFs to Markdown. Most style information is ignored, so the results are essentially plain-text.
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a PDF
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".pdf":
            return None

        return DocumentConverterResult(title=None, text_content=pdfminer_extract_text(local_path))

class DocxConverter(HtmlConverter):
    """
    Converts DOCX files to Markdown. Style information (e.g.m headings) and tables are preserved where possible.
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a DOCX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".docx":
            return None

        result = None
        with open(local_path, "rb") as docx_file:
            result = mammoth.convert_to_html(docx_file)
            html_content = result.value
            result = self._convert(html_content)

        return result

class XlsxConverter(HtmlConverter):
    """
    Converts XLSX files to Markdown, with each sheet presented as a separate Markdown table.
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".xlsx", ".xls"]:
            return None

        sheets = pd.read_excel(local_path, sheet_name=None)
        md_content = ""
        for s in sheets:
            md_content += f"## {s}\n"
            html_content = sheets[s].to_html(index=False)
            md_content += self._convert(html_content).text_content.strip() + "\n\n"

        return DocumentConverterResult(title=None, text_content=md_content.strip())

class PptxConverter(HtmlConverter):
    """
    Converts PPTX files to Markdown. Supports heading, tables and images with alt text.
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a PPTX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".pptx":
            return None

        md_content = ""

        presentation = pptx.Presentation(local_path)
        slide_num = 0
        for slide in presentation.slides:
            slide_num += 1

            md_content += f"\n\n<!-- Slide number: {slide_num} -->\n"

            title = slide.shapes.title
            for shape in slide.shapes:
                # Pictures
                if self._is_picture(shape):
                    # https://github.com/scanny/python-pptx/pull/512#issuecomment-1713100069
                    alt_text = ""
                    try:
                        alt_text = shape._element._nvXxPr.cNvPr.attrib.get("descr", "")
                    except Exception:
                        pass

                    # A placeholder name
                    filename = re.sub(r"\W", "", shape.name) + ".jpg"
                    md_content += "\n![" + (alt_text if alt_text else shape.name) + "](" + filename + ")\n"

                # Tables
                if self._is_table(shape):
                    html_table = "<html><body><table>"
                    first_row = True
                    for row in shape.table.rows:
                        html_table += "<tr>"
                        for cell in row.cells:
                            if first_row:
                                html_table += "<th>" + html.escape(cell.text) + "</th>"
                            else:
                                html_table += "<td>" + html.escape(cell.text) + "</td>"
                        html_table += "</tr>"
                        first_row = False
                    html_table += "</table></body></html>"
                    md_content += "\n" + self._convert(html_table).text_content.strip() + "\n"

                # Text areas
                elif shape.has_text_frame:
                    if shape == title:
                        md_content += "# " + shape.text.lstrip() + "\n"
                    else:
                        md_content += shape.text + "\n"

            md_content = md_content.strip()

            if slide.has_notes_slide:
                md_content += "\n\n### Notes:\n"
                notes_frame = slide.notes_slide.notes_text_frame
                if notes_frame is not None:
                    md_content += notes_frame.text
                md_content = md_content.strip()

        return DocumentConverterResult(title=None, text_content=md_content.strip())

    def _is_picture(self, shape):
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.PICTURE:
            return True
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.PLACEHOLDER:
            if hasattr(shape, "image"):
                return True
        return False

    def _is_table(self, shape):
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.TABLE:
            return True
        return False

class MediaConverter(DocumentConverter):
    """
    Abstract class for multi-modal media (e.g., images and audio)
    """

    def _get_metadata(self, local_path):
        exiftool = shutil.which("exiftool")
        if not exiftool:
            return None
        else:
            try:
                result = subprocess.run([exiftool, "-json", local_path], capture_output=True, text=True).stdout
                return json.loads(result)[0]
            except Exception:
                return None

class WavConverter(MediaConverter):
    """
    Converts WAV files to markdown via extraction of metadata (if `exiftool` is installed), and speech transcription (if `speech_recognition` is installed).
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".wav":
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "Title",
                "Artist",
                "Author",
                "Band",
                "Album",
                "Genre",
                "Track",
                "DateTimeOriginal",
                "CreateDate",
                "Duration",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Transcribe
        try:
            transcript = self._transcribe_audio(local_path)
            md_content += "\n\n### Audio Transcript:\n" + ("[No speech detected]" if transcript == "" else transcript)
        except Exception:
            md_content += "\n\n### Audio Transcript:\nError. Could not transcribe this audio."

        return DocumentConverterResult(title=None, text_content=md_content.strip())

    def _transcribe_audio(self, local_path) -> str:
        recognizer = sr.Recognizer()
        with sr.AudioFile(local_path) as source:
            audio = recognizer.record(source)
            return recognizer.recognize_google(audio).strip()

class Mp3Converter(WavConverter):
    """
    Converts MP3 and M4A files to markdown via extraction of metadata (if `exiftool` is installed), and speech transcription (if `speech_recognition` AND `pydub` are installed).
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a MP3
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".mp3", ".m4a"]:
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "Title",
                "Artist",
                "Author",
                "Band",
                "Album",
                "Genre",
                "Track",
                "DateTimeOriginal",
                "CreateDate",
                "Duration",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Transcribe
        handle, temp_path = tempfile.mkstemp(suffix=".wav")
        os.close(handle)
        try:
            if extension.lower() == ".mp3":
                sound = pydub.AudioSegment.from_mp3(local_path)
            else:
                sound = pydub.AudioSegment.from_file(local_path, format="m4a")
            sound.export(temp_path, format="wav")

            _args = dict()
            _args.update(kwargs)
            _args["file_extension"] = ".wav"

            try:
                transcript = super()._transcribe_audio(temp_path).strip()
                md_content += "\n\n### Audio Transcript:\n" + (
                    "[No speech detected]" if transcript == "" else transcript
                )
            except Exception:
                md_content += "\n\n### Audio Transcript:\nError. Could not transcribe this audio."

        finally:
            os.unlink(temp_path)

        # Return the result
        return DocumentConverterResult(title=None, text_content=md_content.strip())

class ZipConverter(DocumentConverter):
    """
    Extracts ZIP files to a permanent local directory and returns a listing of extracted files.
    """

    def __init__(self, extract_dir: str = "downloads"):
        """
        Initialize with path to extraction directory.

        Args:
            extract_dir: The directory where files will be extracted. Defaults to "downloads"
        """
        self.extract_dir = extract_dir
        # Create the extraction directory if it doesn't exist
        os.makedirs(self.extract_dir, exist_ok=True)

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Bail if not a ZIP file
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".zip":
            return None

        # Verify it's actually a ZIP file
        if not zipfile.is_zipfile(local_path):
            return None

        # Extract all files and build list
        extracted_files = []
        with zipfile.ZipFile(local_path, "r") as zip_ref:
            # Extract all files
            zip_ref.extractall(self.extract_dir)
            # Get list of all files
            for file_path in zip_ref.namelist():
                # Skip directories
                if not file_path.endswith("/"):
                    extracted_files.append(self.extract_dir + "/" + file_path)

        # Sort files for consistent output
        extracted_files.sort()

        # Build the markdown content
        md_content = "Downloaded the following files:\n"
        for file in extracted_files:
            md_content += f"* {file}\n"

        return DocumentConverterResult(title="Extracted Files", text_content=md_content.strip())

class ImageConverter(MediaConverter):
    """
    Converts images to markdown via extraction of metadata (if `exiftool` is installed), OCR (if `easyocr` is installed), and description via a multimodal LLM (if an mlm_client is configured).
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".jpg", ".jpeg", ".png"]:
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "ImageSize",
                "Title",
                "Caption",
                "Description",
                "Keywords",
                "Artist",
                "Author",
                "DateTimeOriginal",
                "CreateDate",
                "GPSPosition",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Try describing the image with GPTV
        mlm_client = kwargs.get("mlm_client")
        mlm_model = kwargs.get("mlm_model")
        if mlm_client is not None and mlm_model is not None:
            md_content += (
                "\n# Description:\n"
                + self._get_mlm_description(local_path, extension, mlm_client, mlm_model, prompt=kwargs.get("mlm_prompt")).strip()
                + "\n"
            )

        return DocumentConverterResult(title=None, text_content=md_content)

    def _get_mlm_description(self, local_path, extension, client, model, prompt=None):
        if prompt is None or prompt.strip() == "":
            prompt = "Write a detailed caption for this image."

        sys.stderr.write(f"MLM Prompt:\n{prompt}\n")

        data_uri = ""
        with open(local_path, "rb") as image_file:
            content_type, encoding = mimetypes.guess_type("_dummy" + extension)
            if content_type is None:
                content_type = "image/jpeg"
            image_base64 = base64.b64encode(image_file.read()).decode("utf-8")
            data_uri = f"data:{content_type};base64,{image_base64}"

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": data_uri,
                        },
                    },
                ],
            }
        ]

        response = client.chat.completions.create(model=model, messages=messages)
        return response.choices[0].message.content

class FileConversionException(Exception):
    pass

class UnsupportedFormatException(Exception):
    pass

class MarkdownConverter:
    """(In preview) An extremely simple text-based document reader, suitable for LLM use.
    This reader will convert common file-types or webpages to Markdown."""

    def __init__(
        self,
        requests_session: requests.Session | None = None,
        mlm_client: Any | None = None,
        mlm_model: Any | None = None,
    ):
        if requests_session is None:
            self._requests_session = requests.Session()
        else:
            self._requests_session = requests_session

        self._mlm_client = mlm_client
        self._mlm_model = mlm_model

        self._page_converters: list[DocumentConverter] = []

        # Register converters for successful browsing operations
        # Later registrations are tried first / take higher priority than earlier registrations
        # To this end, the most specific converters should appear below the most generic converters
        self.register_page_converter(PlainTextConverter())
        self.register_page_converter(HtmlConverter())
        self.register_page_converter(WikipediaConverter())
        self.register_page_converter(YouTubeConverter())
        self.register_page_converter(DocxConverter())
        self.register_page_converter(XlsxConverter())
        self.register_page_converter(PptxConverter())
        self.register_page_converter(WavConverter())
        self.register_page_converter(Mp3Converter())
        self.register_page_converter(ImageConverter())
        self.register_page_converter(ZipConverter())
        self.register_page_converter(PdfConverter())

    def convert(
        self, source: str | requests.Response, **kwargs: Any
    ) -> DocumentConverterResult:  # TODO: deal with kwargs
        """
        Args:
            - source: can be a string representing a path or url, or a requests.response object
            - extension: specifies the file extension to use when interpreting the file. If None, infer from source (path, uri, content-type, etc.)
        """

        # Local path or url
        if isinstance(source, str):
            if source.startswith("http://") or source.startswith("https://") or source.startswith("file://"):
                return self.convert_url(source, **kwargs)
            else:
                return self.convert_local(source, **kwargs)
        # Request response
        elif isinstance(source, requests.Response):
            return self.convert_response(source, **kwargs)

    def convert_local(self, path: str, **kwargs: Any) -> DocumentConverterResult:  # TODO: deal with kwargs
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Get extension alternatives from the path and puremagic
        base, ext = os.path.splitext(path)
        self._append_ext(extensions, ext)
        self._append_ext(extensions, self._guess_ext_magic(path))

        # Convert
        return self._convert(path, extensions, **kwargs)

    # TODO what should stream's type be?
    def convert_stream(self, stream: Any, **kwargs: Any) -> DocumentConverterResult:  # TODO: deal with kwargs
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Save the file locally to a temporary file. It will be deleted before this method exits
        handle, temp_path = tempfile.mkstemp()
        fh = os.fdopen(handle, "wb")
        result = None
        try:
            # Write to the temporary file
            content = stream.read()
            if isinstance(content, str):
                fh.write(content.encode("utf-8"))
            else:
                fh.write(content)
            fh.close()

            # Use puremagic to check for more extension options
            self._append_ext(extensions, self._guess_ext_magic(temp_path))

            # Convert
            result = self._convert(temp_path, extensions, **kwargs)
        # Clean up
        finally:
            try:
                fh.close()
            except Exception:
                pass
            os.unlink(temp_path)

        return result

    def convert_url(self, url: str, **kwargs: Any) -> DocumentConverterResult:  # TODO: fix kwargs type
        # Send a HTTP request to the URL
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"
        response = self._requests_session.get(url, stream=True, headers={"User-Agent": user_agent})
        response.raise_for_status()
        return self.convert_response(response, **kwargs)

    def convert_response(
        self, response: requests.Response, **kwargs: Any
    ) -> DocumentConverterResult:  # TODO fix kwargs type
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Guess from the mimetype
        content_type = response.headers.get("content-type", "").split(";")[0]
        self._append_ext(extensions, mimetypes.guess_extension(content_type))

        # Read the content disposition if there is one
        content_disposition = response.headers.get("content-disposition", "")
        m = re.search(r"filename=([^;]+)", content_disposition)
        if m:
            base, ext = os.path.splitext(m.group(1).strip("\"'"))
            self._append_ext(extensions, ext)

        # Read from the extension from the path
        base, ext = os.path.splitext(urlparse(response.url).path)
        self._append_ext(extensions, ext)

        # Save the file locally to a temporary file. It will be deleted before this method exits
        handle, temp_path = tempfile.mkstemp()
        fh = os.fdopen(handle, "wb")
        result = None
        try:
            # Download the file
            for chunk in response.iter_content(chunk_size=512):
                fh.write(chunk)
            fh.close()

            # Use puremagic to check for more extension options
            self._append_ext(extensions, self._guess_ext_magic(temp_path))

            # Convert
            result = self._convert(temp_path, extensions, url=response.url)
        except Exception as e:
            print(f"Error in converting: {e}")

        # Clean up
        finally:
            try:
                fh.close()
            except Exception:
                pass
            os.unlink(temp_path)

        return result

    def _convert(self, local_path: str, extensions: list[str | None], **kwargs) -> DocumentConverterResult:
        error_trace = ""
        for ext in extensions + [None]:  # Try last with no extension
            for converter in self._page_converters:
                _kwargs = copy.deepcopy(kwargs)

                # Overwrite file_extension appropriately
                if ext is None:
                    if "file_extension" in _kwargs:
                        del _kwargs["file_extension"]
                else:
                    _kwargs.update({"file_extension": ext})

                # Copy any additional global options
                if "mlm_client" not in _kwargs and self._mlm_client is not None:
                    _kwargs["mlm_client"] = self._mlm_client

                if "mlm_model" not in _kwargs and self._mlm_model is not None:
                    _kwargs["mlm_model"] = self._mlm_model

                # If we hit an error log it and keep trying
                try:
                    res = converter.convert(local_path, **_kwargs)
                except Exception:
                    error_trace = ("\n\n" + traceback.format_exc()).strip()

                if res is not None:
                    # Normalize the content
                    res.text_content = "\n".join([line.rstrip() for line in re.split(r"\r?\n", res.text_content)])
                    res.text_content = re.sub(r"\n{3,}", "\n\n", res.text_content)

                    # Todo
                    return res

        # If we got this far without success, report any exceptions
        if len(error_trace) > 0:
            raise FileConversionException(
                f"Could not convert '{local_path}' to Markdown. File type was recognized as {extensions}. While converting the file, the following error was encountered:\n\n{error_trace}"
            )

        # Nothing can handle it!
        raise UnsupportedFormatException(
            f"Could not convert '{local_path}' to Markdown. The formats {extensions} are not supported."
        )

    def _append_ext(self, extensions, ext):
        """Append a unique non-None, non-empty extension to a list of extensions."""
        if ext is None:
            return
        ext = ext.strip()
        if ext == "":
            return
        # if ext not in extensions:
        if True:
            extensions.append(ext)

    def _guess_ext_magic(self, path):
        """Use puremagic (a Python implementation of libmagic) to guess a file's extension based on the first few bytes."""
        # Use puremagic to guess
        try:
            guesses = puremagic.magic_file(path)
            if len(guesses) > 0:
                ext = guesses[0].extension.strip()
                if len(ext) > 0:
                    return ext
        except FileNotFoundError:
            pass
        except IsADirectoryError:
            pass
        except PermissionError:
            pass
        return None

    def register_page_converter(self, converter: DocumentConverter) -> None:
        """Register a page text converter."""
        self._page_converters.insert(0, converter)



================================================
FILE: src/open_deep_research/reformulator.py
================================================
# -*- coding: utf-8 -*-

# Shamelessly stolen from Microsoft Autogen team: thanks to them for this great resource!
# https://github.com/microsoft/autogen/blob/gaia_multiagent_v01_march_1st/autogen/browser_utils.py
import copy

from smolagents.models import MessageRole, Model

def prepare_response(original_task: str, inner_messages, reformulation_model: Model) -> str:
    messages = [
        {
            "role": MessageRole.SYSTEM,
            "content": [
                {
                    "type": "text",
                    "text": f"""Earlier you were asked the following:

{original_task}

Your team then worked diligently to address that request. Read below a transcript of that conversation:""",
                }
            ],
        }
    ]

    # The first message just repeats the question, so remove it
    # if len(inner_messages) > 1:
    #    del inner_messages[0]

    # copy them to this context
    try:
        for message in inner_messages:
            if not message.content:
                continue
            message = copy.deepcopy(message)
            message.role = MessageRole.USER
            messages.append(message)
    except Exception:
        messages += [{"role": MessageRole.ASSISTANT, "content": str(inner_messages)}]

    # ask for the final answer
    messages.append(
        {
            "role": MessageRole.USER,
            "content": [
                {
                    "type": "text",
                    "text": f"""
Read the above conversation and output a FINAL ANSWER to the question. The question is repeated here for convenience:

{original_task}

To output the final answer, use the following template: FINAL ANSWER: [YOUR FINAL ANSWER]
Your FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.
ADDITIONALLY, your FINAL ANSWER MUST adhere to any formatting instructions specified in the original question (e.g., alphabetization, sequencing, units, rounding, decimal places, etc.)
If you are asked for a number, express it numerically (i.e., with digits rather than words), don't use commas, and DO NOT INCLUDE UNITS such as $ or USD or percent signs unless specified otherwise.
If you are asked for a string, don't use articles or abbreviations (e.g. for cities), unless specified otherwise. Don't output any final sentence punctuation such as '.', '!', or '?'.
If you are asked for a comma separated list, apply the above rules depending on whether the elements are numbers or strings.
If you are unable to determine the final answer, output 'FINAL ANSWER: Unable to determine'
""",
                }
            ],
        }
    )

    response = reformulation_model(messages).content

    final_answer = response.split("FINAL ANSWER: ")[-1].strip()
    print("> Reformulated answer: ", final_answer)

    #     if "unable to determine" in final_answer.lower():
    #         messages.append({"role": MessageRole.ASSISTANT, "content": response })
    #         messages.append({"role": MessageRole.USER, "content": [{"type": "text", "text": """
    # I understand that a definitive answer could not be determined. Please make a well-informed EDUCATED GUESS based on the conversation.

    # To output the educated guess, use the following template: EDUCATED GUESS: [YOUR EDUCATED GUESS]
    # Your EDUCATED GUESS should be a number OR as few words as possible OR a comma separated list of numbers and/or strings. DO NOT OUTPUT 'I don't know', 'Unable to determine', etc.
    # ADDITIONALLY, your EDUCATED GUESS MUST adhere to any formatting instructions specified in the original question (e.g., alphabetization, sequencing, units, rounding, decimal places, etc.)
    # If you are asked for a number, express it numerically (i.e., with digits rather than words), don't use commas, and don't include units such as $ or percent signs unless specified otherwise.
    # If you are asked for a string, don't use articles or abbreviations (e.g. cit for cities), unless specified otherwise. Don't output any final sentence punctuation such as '.', '!', or '?'.
    # If you are asked for a comma separated list, apply the above rules depending on whether the elements are numbers or strings.
    # """.strip()}]})

    #         response = model(messages).content
    #         print("\n>>>Making an educated guess.\n", response)
    #         final_answer = response.split("EDUCATED GUESS: ")[-1].strip()
    return final_answer



================================================
FILE: src/open_deep_research/run_agents.py
================================================
# -*- coding: utf-8 -*-

import json
import os
import shutil
import textwrap
from pathlib import Path

def get_image_description(file_name: str, question: str, visual_inspection_tool) -> str:
    prompt = f"""Write a caption of 5 sentences for this image. Pay special attention to any details that might be useful for someone answering the following question:
{question}. But do not try to answer the question directly!
Do not add any information that is not present in the image."""
    return visual_inspection_tool(image_path=file_name, question=prompt)

def get_document_description(file_path: str, question: str, document_inspection_tool) -> str:
    prompt = f"""Write a caption of 5 sentences for this document. Pay special attention to any details that might be useful for someone answering the following question:
{question}. But do not try to answer the question directly!
Do not add any information that is not present in the document."""
    return document_inspection_tool.forward_initial_exam_mode(file_path=file_path, question=prompt)

def get_single_file_description(file_path: str, question: str, visual_inspection_tool, document_inspection_tool):
    file_extension = file_path.split(".")[-1]
    if file_extension in ["png", "jpg", "jpeg"]:
        file_description = f" - Attached image: {file_path}"
        file_description += (
            f"\n     -> Image description: {get_image_description(file_path, question, visual_inspection_tool)}"
        )
        return file_description
    elif file_extension in ["pdf", "xls", "xlsx", "docx", "doc", "xml"]:
        image_path = file_path.split(".")[0] + ".png"
        if os.path.exists(image_path):
            description = get_image_description(image_path, question, visual_inspection_tool)
            file_path = image_path
        else:
            description = get_document_description(file_path, question, document_inspection_tool)
        file_description = f" - Attached document: {file_path}"
        file_description += f"\n     -> File description: {description}"
        return file_description
    elif file_extension in ["mp3", "m4a", "wav"]:
        return f" - Attached audio: {file_path}"
    else:
        return f" - Attached file: {file_path}"

def get_zip_description(file_path: str, question: str, visual_inspection_tool, document_inspection_tool):
    folder_path = file_path.replace(".zip", "")
    os.makedirs(folder_path, exist_ok=True)
    shutil.unpack_archive(file_path, folder_path)

    prompt_use_files = ""
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            prompt_use_files += "\n" + textwrap.indent(get_single_file_description(file_path, question, visual_inspection_tool, document_inspection_tool), prefix="    ")
    return prompt_use_files




================================================
FILE: src/open_deep_research/text_inspector_tool.py
================================================
# -*- coding: utf-8 -*-

from smolagents import Tool
from smolagents.models import Model

class TextInspectorTool(Tool):
    name = "inspect_file_as_text"
    description = """
You cannot load files yourself: instead call this tool to read a file as markdown text and ask questions about it.
This tool handles the following file extensions: [".html", ".htm", ".xlsx", ".pptx", ".wav", ".mp3", ".m4a", ".flac", ".pdf", ".docx"], and all other types of text files. IT DOES NOT HANDLE IMAGES."""

    inputs = {
        "file_path": {
            "description": "The path to the file you want to read as text. Must be a '.something' file, like '.pdf'. If it is an image, use the visualizer tool instead! DO NOT use this tool for an HTML webpage: use the web_search tool instead!",
            "type": "string",
        },
        "question": {
            "description": "[Optional]: Your question, as a natural language sentence. Provide as much context as possible. Do not pass this parameter if you just want to directly return the content of the file.",
            "type": "string",
            "nullable": True,
        },
    }
    output_type = "string"

    def __init__(self, model: Model = None, text_limit: int = 100000):
        super().__init__()
        self.model = model
        self.text_limit = text_limit
        from src.open_deep_research.mdconvert import MarkdownConverter

        self.md_converter = MarkdownConverter()

    def forward_initial_exam_mode(self, file_path, question):
        from smolagents.models import MessageRole

        result = self.md_converter.convert(file_path)

        if file_path[-4:] in [".png", ".jpg"]:
            raise Exception("Cannot use inspect_file_as_text tool with images: use visualizer instead!")

        if ".zip" in file_path:
            return result.text_content

        if not question:
            return result.text_content

        if len(result.text_content) < 4000:
            return "Document content: " + result.text_content

        messages = [
            {
                "role": MessageRole.SYSTEM,
                "content": [
                    {
                        "type": "text",
                        "text": "Here is a file:\n### "
                        + str(result.title)
                        + "\n\n"
                        + result.text_content[: self.text_limit],
                    }
                ],
            },
            {
                "role": MessageRole.USER,
                "content": [
                    {
                        "type": "text",
                        "text": "Now please write a short, 5 sentence caption for this document, that could help someone asking this question: "
                        + question
                        + "\n\nDon't answer the question yourself! Just provide useful notes on the document",
                    }
                ],
            },
        ]
        return self.model(messages).content

    def forward(self, file_path, question: str | None = None) -> str:
        from smolagents.models import MessageRole

        result = self.md_converter.convert(file_path)

        if file_path[-4:] in [".png", ".jpg"]:
            raise Exception("Cannot use inspect_file_as_text tool with images: use visualizer instead!")

        if ".zip" in file_path:
            return result.text_content

        if not question:
            return result.text_content

        messages = [
            {
                "role": MessageRole.SYSTEM,
                "content": [
                    {
                        "type": "text",
                        "text": "You will have to write a short caption for this file, then answer this question:"
                        + question,
                    }
                ],
            },
            {
                "role": MessageRole.USER,
                "content": [
                    {
                        "type": "text",
                        "text": "Here is the complete file:\n### "
                        + str(result.title)
                        + "\n\n"
                        + result.text_content[: self.text_limit],
                    }
                ],
            },
            {
                "role": MessageRole.USER,
                "content": [
                    {
                        "type": "text",
                        "text": "Now answer the question below. Use these three headings: '1. Short answer', '2. Extremely detailed answer', '3. Additional Context on the document and question asked'."
                        + question,
                    }
                ],
            },
        ]
        return self.model(messages).content



================================================
FILE: src/open_deep_research/text_web_browser.py
================================================
# -*- coding: utf-8 -*-

# Shamelessly stolen from Microsoft Autogen team: thanks to them for this great resource!
# https://github.com/microsoft/autogen/blob/gaia_multiagent_v01_march_1st/autogen/browser_utils.py
import mimetypes
import os
import pathlib
import re
import time
import uuid
from typing import Any
from urllib.parse import unquote, urljoin, urlparse

import pathvalidate
import requests
from serpapi import GoogleSearch

from smolagents import Tool

from open_deep_research.cookies import COOKIES
from open_deep_research.mdconvert import FileConversionException, MarkdownConverter, UnsupportedFormatException

class SimpleTextBrowser:
    """(In preview) An extremely simple text-based web browser comparable to Lynx. Suitable for Agentic use."""

    def __init__(
        self,
        start_page: str | None = None,
        viewport_size: int | None = 1024 * 8,
        downloads_folder: str | None | None = None,
        serpapi_key: str | None | None = None,
        request_kwargs: dict[str, Any] | None | None = None,
    ):
        self.start_page: str = start_page if start_page else "about:blank"
        self.viewport_size = viewport_size  # Applies only to the standard uri types
        self.downloads_folder = downloads_folder
        self.history: list[tuple[str, float]] = list()
        self.page_title: str | None = None
        self.viewport_current_page = 0
        self.viewport_pages: list[tuple[int, int]] = list()
        self.set_address(self.start_page)
        self.serpapi_key = serpapi_key
        self.request_kwargs = request_kwargs
        self.request_kwargs["cookies"] = COOKIES
        self._mdconvert = MarkdownConverter()
        self._page_content: str = ""

        self._find_on_page_query: str | None = None
        self._find_on_page_last_result: int | None = None  # Location of the last result

    @property
    def address(self) -> str:
        """Return the address of the current page."""
        return self.history[-1][0]

    def set_address(self, uri_or_path: str, filter_year: int | None = None) -> None:
        # TODO: Handle anchors
        self.history.append((uri_or_path, time.time()))

        # Handle special URIs
        if uri_or_path == "about:blank":
            self._set_page_content("")
        elif uri_or_path.startswith("google:"):
            self._serpapi_search(uri_or_path[len("google:") :].strip(), filter_year=filter_year)
        else:
            if (
                not uri_or_path.startswith("http:")
                and not uri_or_path.startswith("https:")
                and not uri_or_path.startswith("file:")
            ):
                if len(self.history) > 1:
                    prior_address = self.history[-2][0]
                    uri_or_path = urljoin(prior_address, uri_or_path)
                    # Update the address with the fully-qualified path
                    self.history[-1] = (uri_or_path, self.history[-1][1])
            self._fetch_page(uri_or_path)

        self.viewport_current_page = 0
        self.find_on_page_query = None
        self.find_on_page_viewport = None

    @property
    def viewport(self) -> str:
        """Return the content of the current viewport."""
        bounds = self.viewport_pages[self.viewport_current_page]
        return self.page_content[bounds[0] : bounds[1]]

    @property
    def page_content(self) -> str:
        """Return the full contents of the current page."""
        return self._page_content

    def _set_page_content(self, content: str) -> None:
        """Sets the text content of the current page."""
        self._page_content = content
        self._split_pages()
        if self.viewport_current_page >= len(self.viewport_pages):
            self.viewport_current_page = len(self.viewport_pages) - 1

    def page_down(self) -> None:
        self.viewport_current_page = min(self.viewport_current_page + 1, len(self.viewport_pages) - 1)

    def page_up(self) -> None:
        self.viewport_current_page = max(self.viewport_current_page - 1, 0)

    def find_on_page(self, query: str) -> str | None:
        """Searches for the query from the current viewport forward, looping back to the start if necessary."""

        # Did we get here via a previous find_on_page search with the same query?
        # If so, map to find_next
        if query == self._find_on_page_query and self.viewport_current_page == self._find_on_page_last_result:
            return self.find_next()

        # Ok it's a new search start from the current viewport
        self._find_on_page_query = query
        viewport_match = self._find_next_viewport(query, self.viewport_current_page)
        if viewport_match is None:
            self._find_on_page_last_result = None
            return None
        else:
            self.viewport_current_page = viewport_match
            self._find_on_page_last_result = viewport_match
            return self.viewport

    def find_next(self) -> str | None:
        """Scroll to the next viewport that matches the query"""

        if self._find_on_page_query is None:
            return None

        starting_viewport = self._find_on_page_last_result
        if starting_viewport is None:
            starting_viewport = 0
        else:
            starting_viewport += 1
            if starting_viewport >= len(self.viewport_pages):
                starting_viewport = 0

        viewport_match = self._find_next_viewport(self._find_on_page_query, starting_viewport)
        if viewport_match is None:
            self._find_on_page_last_result = None
            return None
        else:
            self.viewport_current_page = viewport_match
            self._find_on_page_last_result = viewport_match
            return self.viewport

    def _find_next_viewport(self, query: str, starting_viewport: int) -> int | None:
        """Search for matches between the starting viewport looping when reaching the end."""

        if query is None:
            return None

        # Normalize the query, and convert to a regular expression
        nquery = re.sub(r"\*", "__STAR__", query)
        nquery = " " + (" ".join(re.split(r"\W+", nquery))).strip() + " "
        nquery = nquery.replace(" __STAR__ ", "__STAR__ ")  # Merge isolated stars with prior word
        nquery = nquery.replace("__STAR__", ".*").lower()

        if nquery.strip() == "":
            return None

        idxs = list()
        idxs.extend(range(starting_viewport, len(self.viewport_pages)))
        idxs.extend(range(0, starting_viewport))

        for i in idxs:
            bounds = self.viewport_pages[i]
            content = self.page_content[bounds[0] : bounds[1]]

            # TODO: Remove markdown links and images
            ncontent = " " + (" ".join(re.split(r"\W+", content))).strip().lower() + " "
            if re.search(nquery, ncontent):
                return i

        return None

    def visit_page(self, path_or_uri: str, filter_year: int | None = None) -> str:
        """Update the address, visit the page, and return the content of the viewport."""
        self.set_address(path_or_uri, filter_year=filter_year)
        return self.viewport

    def _split_pages(self) -> None:
        # Do not split search results
        if self.address.startswith("google:"):
            self.viewport_pages = [(0, len(self._page_content))]
            return

        # Handle empty pages
        if len(self._page_content) == 0:
            self.viewport_pages = [(0, 0)]
            return

        # Break the viewport into pages
        self.viewport_pages = []
        start_idx = 0
        while start_idx < len(self._page_content):
            end_idx = min(start_idx + self.viewport_size, len(self._page_content))  # type: ignore[operator]
            # Adjust to end on a space
            while end_idx < len(self._page_content) and self._page_content[end_idx - 1] not in [" ", "\t", "\r", "\n"]:
                end_idx += 1
            self.viewport_pages.append((start_idx, end_idx))
            start_idx = end_idx

    def _serpapi_search(self, query: str, filter_year: int | None = None) -> None:
        if self.serpapi_key is None:
            raise ValueError("Missing SerpAPI key.")

        params = {
            "engine": "google",
            "q": query,
            "api_key": self.serpapi_key,
        }
        if filter_year is not None:
            params["tbs"] = f"cdr:1,cd_min:01/01/{filter_year},cd_max:12/31/{filter_year}"

        search = GoogleSearch(params)
        results = search.get_dict()
        self.page_title = f"{query} - Search"
        if "organic_results" not in results.keys():
            raise Exception(f"No results found for query: '{query}'. Use a less specific query.")
        if len(results["organic_results"]) == 0:
            year_filter_message = f" with filter year={filter_year}" if filter_year is not None else ""
            self._set_page_content(f"No results found for '{query}'{year_filter_message}. Try with a more general query, or remove the year filter.")
            return

        def _prev_visit(url):
            for i in range(len(self.history) - 1, -1, -1):
                if self.history[i][0] == url:
                    return f"You previously visited this page {round(time.time() - self.history[i][1])} seconds ago.\n"
            return ""

        web_snippets: list[str] = list()
        idx = 0
        if "organic_results" in results:
            for page in results["organic_results"]:
                idx += 1
                date_published = ""
                if "date" in page:
                    date_published = "\nDate published: " + page["date"]

                source = ""
                if "source" in page:
                    source = "\nSource: " + page["source"]

                snippet = ""
                if "snippet" in page:
                    snippet = "\n" + page["snippet"]

                redacted_version = f"{idx}. [{page['title']}]({page['link']}){date_published}{source}\n{_prev_visit(page['link'])}{snippet}"

                redacted_version = redacted_version.replace("Your browser can't play this video.", "")
                web_snippets.append(redacted_version)

        content = (
            f"A Google search for '{query}' found {len(web_snippets)} results:\n\n## Web Results\n"
            + "\n\n".join(web_snippets)
        )

        self._set_page_content(content)

    def _fetch_page(self, url: str) -> None:
        download_path = ""
        try:
            if url.startswith("file://"):
                download_path = os.path.normcase(os.path.normpath(unquote(url[7:])))
                res = self._mdconvert.convert_local(download_path)
                self.page_title = res.title
                self._set_page_content(res.text_content)
            else:
                # Prepare the request parameters
                request_kwargs = self.request_kwargs.copy() if self.request_kwargs is not None else {}
                request_kwargs["stream"] = True

                # Send a HTTP request to the URL
                response = requests.get(url, **request_kwargs)
                response.raise_for_status()

                # If the HTTP request was successful
                content_type = response.headers.get("content-type", "")

                # Text or HTML
                if "text/" in content_type.lower():
                    res = self._mdconvert.convert_response(response)
                    self.page_title = res.title
                    self._set_page_content(res.text_content)
                # A download
                else:
                    # Try producing a safe filename
                    fname = None
                    download_path = None
                    try:
                        fname = pathvalidate.sanitize_filename(os.path.basename(urlparse(url).path)).strip()
                        download_path = os.path.abspath(os.path.join(self.downloads_folder, fname))

                        suffix = 0
                        while os.path.exists(download_path) and suffix < 1000:
                            suffix += 1
                            base, ext = os.path.splitext(fname)
                            new_fname = f"{base}__{suffix}{ext}"
                            download_path = os.path.abspath(os.path.join(self.downloads_folder, new_fname))

                    except NameError:
                        pass

                    # No suitable name, so make one
                    if fname is None:
                        extension = mimetypes.guess_extension(content_type)
                        if extension is None:
                            extension = ".download"
                        fname = str(uuid.uuid4()) + extension
                        download_path = os.path.abspath(os.path.join(self.downloads_folder, fname))

                    # Open a file for writing
                    with open(download_path, "wb") as fh:
                        for chunk in response.iter_content(chunk_size=512):
                            fh.write(chunk)

                    # Render it
                    local_uri = pathlib.Path(download_path).as_uri()
                    self.set_address(local_uri)

        except UnsupportedFormatException as e:
            print(e)
            self.page_title = ("Download complete.",)
            self._set_page_content(f"# Download complete\n\nSaved file to '{download_path}'")
        except FileConversionException as e:
            print(e)
            self.page_title = ("Download complete.",)
            self._set_page_content(f"# Download complete\n\nSaved file to '{download_path}'")
        except FileNotFoundError:
            self.page_title = "Error 404"
            self._set_page_content(f"## Error 404\n\nFile not found: {download_path}")
        except requests.exceptions.RequestException as request_exception:
            try:
                self.page_title = f"Error {response.status_code}"

                # If the error was rendered in HTML we might as well render it
                content_type = response.headers.get("content-type", "")
                if content_type is not None and "text/html" in content_type.lower():
                    res = self._mdconvert.convert(response)
                    self.page_title = f"Error {response.status_code}"
                    self._set_page_content(f"## Error {response.status_code}\n\n{res.text_content}")
                else:
                    text = ""
                    for chunk in response.iter_content(chunk_size=512, decode_unicode=True):
                        text += chunk
                    self.page_title = f"Error {response.status_code}"
                    self._set_page_content(f"## Error {response.status_code}\n\n{text}")
            except NameError:
                self.page_title = "Error"
                self._set_page_content(f"## Error\n\n{str(request_exception)}")

    def _state(self) -> tuple[str, str]:
        header = f"Address: {self.address}\n"
        if self.page_title is not None:
            header += f"Title: {self.page_title}\n"

        current_page = self.viewport_current_page
        total_pages = len(self.viewport_pages)

        address = self.address
        for i in range(len(self.history) - 2, -1, -1):  # Start from the second last
            if self.history[i][0] == address:
                header += f"You previously visited this page {round(time.time() - self.history[i][1])} seconds ago.\n"
                break

        header += f"Viewport position: Showing page {current_page + 1} of {total_pages}.\n"
        return (header, self.viewport)

class VisitTool(Tool):
    name = "visit_page"
    description = "Visit a webpage at a given URL and return its text. Given a url to a YouTube video, this returns the transcript."
    inputs = {"url": {"type": "string", "description": "The relative or absolute url of the webpage to visit."}}
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self, url: str) -> str:
        self.browser.visit_page(url)
        header, content = self.browser._state()
        return header.strip() + "\n=======================\n" + content

class ArchiveSearchTool(Tool):
    name = "find_archived_url"
    description = "Given a url, searches the Wayback Machine and returns the archived version of the url that's closest in time to the desired date."
    inputs = {
        "url": {"type": "string", "description": "The url you need the archive for."},
        "date": {
            "type": "string",
            "description": "The date that you want to find the archive for. Give this date in the format 'YYYYMMDD', for instance '27 June 2008' is written as '20080627'.",
        },
    }
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self, url, date) -> str:
        import requests

        no_timestamp_url = f"https://archive.org/wayback/available?url={url}"
        archive_url = no_timestamp_url + f"&timestamp={date}"
        response = requests.get(archive_url).json()
        response_notimestamp = requests.get(no_timestamp_url).json()
        if "archived_snapshots" in response and "closest" in response["archived_snapshots"]:
            closest = response["archived_snapshots"]["closest"]
            print("Archive found!", closest)

        elif "archived_snapshots" in response_notimestamp and "closest" in response_notimestamp["archived_snapshots"]:
            closest = response_notimestamp["archived_snapshots"]["closest"]
            print("Archive found!", closest)
        else:
            raise Exception(f"Your {url=} was not archived on Wayback Machine, try a different url.")
        target_url = closest["url"]
        self.browser.visit_page(target_url)
        header, content = self.browser._state()
        return (
            f"Web archive for url {url}, snapshot taken at date {closest['timestamp'][:8]}:\n"
            + header.strip()
            + "\n=======================\n"
            + content
        )

class PageUpTool(Tool):
    name = "page_up"
    description = "Scroll the viewport UP one page-length in the current webpage and return the new viewport content."
    inputs = {}
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self) -> str:
        self.browser.page_up()
        header, content = self.browser._state()
        return header.strip() + "\n=======================\n" + content

class PageDownTool(Tool):
    name = "page_down"
    description = (
        "Scroll the viewport DOWN one page-length in the current webpage and return the new viewport content."
    )
    inputs = {}
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self) -> str:
        self.browser.page_down()
        header, content = self.browser._state()
        return header.strip() + "\n=======================\n" + content

class FinderTool(Tool):
    name = "find_on_page_ctrl_f"
    description = "Scroll the viewport to the first occurrence of the search string. This is equivalent to Ctrl+F."
    inputs = {
        "search_string": {
            "type": "string",
            "description": "The string to search for on the page. This search string supports wildcards like '*'",
        }
    }
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self, search_string: str) -> str:
        find_result = self.browser.find_on_page(search_string)
        header, content = self.browser._state()

        if find_result is None:
            return (
                header.strip()
                + f"\n=======================\nThe search string '{search_string}' was not found on this page."
            )
        else:
            return header.strip() + "\n=======================\n" + content

class FindNextTool(Tool):
    name = "find_next"
    description = "Scroll the viewport to next occurrence of the search string. This is equivalent to finding the next match in a Ctrl+F search."
    inputs = {}
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self) -> str:
        find_result = self.browser.find_next()
        header, content = self.browser._state()

        if find_result is None:
            return header.strip() + "\n=======================\nThe search string was not found on this page."
        else:
            return header.strip() + "\n=======================\n" + content



================================================
FILE: src/open_deep_research/visual_qa.py
================================================
# -*- coding: utf-8 -*-

import base64
import mimetypes
import os
import uuid

import PIL.Image
import requests
from dotenv import load_dotenv

from smolagents import tool

load_dotenv(override=True)

# Function to encode the image
def encode_image(image_path):
    if image_path.startswith("http"):
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"
        request_kwargs = {
            "headers": {"User-Agent": user_agent},
            "stream": True,
        }

        # Send a HTTP request to the URL
        response = requests.get(image_path, **request_kwargs)
        response.raise_for_status()
        content_type = response.headers.get("content-type", "")

        extension = mimetypes.guess_extension(content_type)
        if extension is None:
            extension = ".download"

        fname = str(uuid.uuid4()) + extension
        download_path = os.path.abspath(os.path.join("downloads", fname))

        with open(download_path, "wb") as fh:
            for chunk in response.iter_content(chunk_size=512):
                fh.write(chunk)

        image_path = download_path

    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

@tool
def visualizer(image_path: str, question: str | None = None) -> str:
    """A tool that can answer questions about attached images.

    Args:
        image_path: The path to the image on which to answer the question. This should be a local path to downloaded image.
        question: The question to answer.
    """
    import mimetypes
    import os

    import requests

    from src.open_deep_research.visual_qa import encode_image

    add_note = False
    if not question:
        add_note = True
        question = "Please write a detailed caption for this image."
    if not isinstance(image_path, str):
        raise Exception("You should provide at least `image_path` string argument to this tool!")

    mime_type, _ = mimetypes.guess_type(image_path)
    base64_image = encode_image(image_path)

    payload = {
        "model": "gpt-4o",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}},
                ],
            }
        ],
        "max_tokens": 1000,
    }
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}"}
    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)
    try:
        output = response.json()["choices"][0]["message"]["content"]
    except Exception:
        raise Exception(f"Response format unexpected: {response.json()}")

    if add_note:
        output = f"You did not provide a particular question, so here is a detailed caption for the image: {output}"

    return output



================================================
FILE: tests/test_agent.py
================================================
# -*- coding: utf-8 -*-

"""
Test for research agent using DuckDuckGo search engine.

This test validates the research agent's ability to search for and analyze
academic papers using DuckDuckGo as the search backend.
"""

import os
import threading
from dotenv import load_dotenv
from huggingface_hub import login

from smolagents import (
    CodeAgent,
    DuckDuckGoSearchTool,
    LiteLLMModel,
    ToolCallingAgent,
)
from open_deep_research.text_inspector_tool import TextInspectorTool
from open_deep_research.text_web_browser import (
    ArchiveSearchTool,
    FinderTool,
    FindNextTool,
    PageDownTool,
    PageUpTool,
    SimpleTextBrowser,
    VisitTool,
)
from open_deep_research.visual_qa import visualizer


def test_research_agent_duckduckgo():
    """Test research agent with DuckDuckGo search for latest Pasquale Minervini paper on arxiv."""
    
    # Load environment variables
    load_dotenv(override=True)
    if os.getenv("HF_TOKEN"):
        login(os.getenv("HF_TOKEN"))

    # Configuration
    model_id = "openai/qwen/qwen3-coder-30b"
    api_base = "http://localhost:1234/v1"
    api_key = "api-key"
    
    custom_role_conversions = {"tool-call": "assistant", "tool-response": "user"}
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"
    
    browser_config = {
        "viewport_size": 1024 * 5,
        "downloads_folder": "downloads_folder",
        "request_kwargs": {
            "headers": {"User-Agent": user_agent},
            "timeout": 300,
        },
        "serpapi_key": os.getenv("SERPAPI_API_KEY"),
    }
    
    os.makedirs(f"./{browser_config['downloads_folder']}", exist_ok=True)
    
    # Create model
    model = LiteLLMModel(
        model_id=model_id,
        api_base=api_base,
        api_key=api_key,
        custom_role_conversions=custom_role_conversions,
        max_completion_tokens=8192,
    )
    
    # Set up browser and tools
    text_limit = 100000
    browser = SimpleTextBrowser(**browser_config)
    
    # Create web tools with DuckDuckGo search
    web_tools = [
        DuckDuckGoSearchTool(),
        VisitTool(browser),
        PageUpTool(browser),
        PageDownTool(browser),
        FinderTool(browser),
        FindNextTool(browser),
        ArchiveSearchTool(browser),
        TextInspectorTool(model, text_limit),
    ]
    
    # Create search agent
    search_agent = ToolCallingAgent(
        model=model,
        tools=web_tools,
        max_steps=20,
        verbosity_level=2,
        planning_interval=4,
        name="search_agent",
        description="""A team member that will search the internet to answer your question.
Ask him for all your questions that require browsing the web.
Provide him as much context as possible, in particular if you need to search on a specific timeframe!
And don't hesitate to provide him with a complex search task, like finding a difference between two webpages.
Your request must be a real sentence, not a google search! Like "Find me this information (...)" rather than a few keywords.
""",
        provide_run_summary=True,
    )
    
    # Add custom prompt template
    search_agent.prompt_templates["managed_agent"]["task"] += """You can navigate to .txt online files.
If a non-html page is in another format, especially .pdf or a Youtube video, use tool 'inspect_file_as_text' to inspect it.
Additionally, if after some searching you find out that you need more information to answer the question, you can use `final_answer` with your request for clarification as argument to request for more information."""
    
    # Create manager agent
    manager_agent = CodeAgent(
        model=model,
        tools=[visualizer, TextInspectorTool(model, text_limit)],
        max_steps=12,
        verbosity_level=2,
        planning_interval=4,
        managed_agents=[search_agent],
        additional_authorized_imports=["*"],
    )
    
    # Run the research task
    question = "What's the latest paper by Pasquale Minervini on arxiv?"
    answer = manager_agent.run(question)
    
    # Assertions
    assert isinstance(answer, str)
    assert len(answer) > 0
    
    # Check that the answer contains relevant information about papers/research
    answer_lower = answer.lower()
    assert any(keyword in answer_lower for keyword in ["paper", "arxiv", "research", "publication", "minervini"])
    
    print(f"Research completed successfully. Answer: {answer}")


================================================
FILE: .claude/settings.local.json
================================================
{
  "permissions": {
    "allow": [
      "Bash(git push:*)",
      "Bash(python:*)"
    ],
    "deny": []
  }
}

